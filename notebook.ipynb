{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d10647c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain \n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3a3a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ee99d",
   "metadata": {},
   "source": [
    "## Stuff Type \n",
    "- metode summarization yang dimana langsung mengirimkan documents ke prompt dan melakukan summary\n",
    "- cocok untuk file document yang kecil (<4000 tokens atau file dengan ukuran <200kb(plain text) dan <500kb(pdf))\n",
    "- tidak untuk document dengan ukuran yang besar (>4000 tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c0a5b",
   "metadata": {},
   "source": [
    "### Load Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1ed6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load document \n",
    "docs = PyPDFLoader('data\\A Study on Backpropagation in Artificial Neural Networks.pdf').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bca37a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\nhttp://dx.doi.org/10.21742/ajnnia.2020.4.1.03 \\n \\n \\nPrint ISSN: 2207-8738, eISSN: 2207-8746 AJNNIA \\nCopyright ‚ìí 2020 Global Vision Press (GV Press) \\nA Study on Backpropagation in Artificial Neural Networks \\n \\n \\nCh Sekhar1 and P Sai Meghana2 \\nDepartment of CSE, VIIT(A), AP, India University, India \\n1sekhar1203@gmail.com, 2palavalasasaimeghana@gmail.com \\nAbstract \\nInnovation assumes essential job nowadays in human life to limit the manual work. \\nExecution and exactness with innovation will be high. The Backpropagation neural \\nframework is multilayered, feedforward neural framewo rk and is by a full edge the most \\nextensively utilized. It is moreover seen as one of the least demanding and most wide systems \\nused for managed planning of multilayered neural systems. Backpropagation works by \\napproximating the non -direct association betw een the data and the yield by changing the \\nweight regards inside. It can furthermore be summarized for the data that is rejected from the \\nplanning structures (perceptive limits).  \\n \\nKeywords:  Backpropagation, ANN, Neuron, Nervous system, MLP, Feedforward networks  \\n \\n1. Introduction \\nA neural system is a gathering of associated I/O units where every association has a weight -\\nrelated with its PC programs. It encourages you to develop prescient models from enormous \\ndatabases. This model expands upon the human sen sory system. It encourages you to lead \\npicture understanding, human learning, speech recognition, and so on.  \\nBackpropagation is the embodiment of neural net preparing. It is the technique for \\ntweaking loads of a neural net dependent on the error value got  in the past epoch (i.e., \\nemphasis). Legitimate tuning of the loads permits you to lessen error value and to make the \\nmodel dependable by expanding its speculation. Backpropagation is a compact structure for \\n‚Äúbackward propagation of errors. ‚Äù It is a standa rd technique for preparing artificial neural \\nsystems [1]. This technique assists with computing the inclination of a misfortune work \\nregarding all the loads in the s ystem. Backpropagation recipes from essential standards and \\ngenuine qualities. The neural system uses three information neurons, one shrouded layer with \\ntwo neurons, and a yield layer with two neurons.  \\n \\n2. The literature on back propagation \\nDuring the feed forward computation neural networks, the result output value is not near to \\ntarget or teacher output value. There is a difference between target, and actual feed  forward \\nvalues lead error value. The neural network model tries to give the best predicti on output will \\ngood tolerance for that we have to minimize the error rate. This can be done with \\nBackpropagation \\nMilestone of Backpropagation: \\n                                                           \\nArticle history:  \\nReceived (April 26, 2020), Review Result (May 29, 2020), Accepted (July 10, 2020)'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='A Study on Backpropagation in Artificial Neural Networks \\n \\n \\n \\n22 Ch Sekhar and P Sai Meghana \\n\\uf0b7 In 1961, the essentials idea of ceaseless Backpropagation was inferred with regards to \\ncontrol hypothesis by J. Kelly, Henry Arthur, and E. Bryson.   \\n\\uf0b7 In 1969, Bryson and Ho gave a multi -orchestrate dynamic system improvement \\nmethod.  \\n\\uf0b7 In 1982, Hopfield brought his idea of a neural framework.  \\n\\uf0b7 In 1986, by the effort of David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams, \\nBackpropagation got affirmation.   \\n\\uf0b7 In 1993, Wan was the primary individual to win a stellar example acknowledgement \\nchallenge with the assistance of the backpropagation strategy.  \\n \\n3. Backpropagation algorithm and computational process \\n \\n \\n \\nFigure 1. Flow diagram of the working mechanism of backpropagation \\nThe above [Figure 1] shows that the everyday workflows of the backpropagation process \\nmechanism.  During the backwards propagation, these computation activities will happen as \\nmentioned below. \\n\\uf0b7 Find Error rate: Here we need to calculate the model output with actual output \\n\\uf0b7 Minimum Error: Cross verifying whether the error is minimized or not. \\n\\uf0b7 Update the Weights: The error is more than the acceptable range then, update the \\nweights and biases. After that, again check the error. Repeat the process until the \\nerror becomes low. \\n\\uf0b7 Neural Network Model:  Once the error rate was acceptable range then the model is \\nready to use for forecasting the data \\nThe generalized workflow and stepwise computation in Backpropagation g iven as pseudo-\\ncode as follows: \\nNeural Network \\nModel \\nFind Error rate \\nErr is \\nMin? \\n Update the Weights \\nNN Model is ready \\nWeights'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\n \\n \\nCopyright ¬© 2020 Global Vision Press (GV Press) 23 \\n \\n \\n \\nFigure 2. Feed forward ANN'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='A Study on Backpropagation in Artificial Neural Networks \\n \\n \\n \\n24 Ch Sekhar and P Sai Meghana \\nThe above [Figure 2 ] shows the feedforward artificial neural network contains Input, \\nHidden and Output layers. Each layer contains two nodes with respective weights. All nodes \\nare fully connected model, including the bias node.  \\nNotions of the above network as follows: \\n\\uf09f X1, X2: Input Nodes \\n\\uf09f H1, H2: Hidden Layer Nodes with net out from respective inputs \\n\\uf09f HA1, HA2: Hidden Layer Nodes with activation output  \\n\\uf09f O1, O2: Output Layer Nodes with net out from respective inputs \\n\\uf09f OA1, OA2: Output Layer Nodes with activation output \\n\\uf09f W1 to W8: Weights of respective layers from input to output \\n\\uf09f B1, B2: Bias Nodes for Hidden and Output layers respectively \\n \\n4. Working with backpropagation  \\nThe Following steps are followed involved during Backpropagation of above feedforwa rd \\nnetwork.  \\n \\nStep 1 \\nThe input and target values for this problem are  X1=1, X2=2, I and target values t1 =0.5  \\nand t2=0.05. The weights of the network need to be randomly chosen within the range of 0 to \\n1.  Here we initialize weights as shown in the figure above for understanding the process.  \\n \\nStep 2 \\nFrom the Step1, we got the inputs and respective weights, as it was a feed-forward network. \\nThe neuron will send to next neuron, i.e. hidden layer neuron. As it was fully connected \\nnetwork, each node/neuron will receive inputs from all the nodes/neurons of the input layer.  \\nNow here we are going to calculate the summation output of at each node of the hidden \\nlayer as follows,  \\nùêª1 = (ùëä1 √ó ùëã1) + (ùëä3 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (1) \\nùêª2 = (ùëä2 √ó ùëã1) + (ùëä4 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (2) \\nFrom the above equations, we are going to calculate feed -forward computation from input \\nto hidden and hidden to output layers respectively   \\nInput to the hidden layer \\nùêª1 = (1 √ó 0.1) + (2 √ó 0.3) + (1 √ó 0.5) = 1.2    (3) \\nùêª2 = (1 √ó 0.2) + (2 √ó 0.4) + (1 √ó 0.5) = 1.5    (4) \\nApplying activation function for both hidden nodes, here we are using sigmoid activation \\nfunctions.'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\n \\n \\nCopyright ¬© 2020 Global Vision Press (GV Press) 25 \\nS= 1/ (1+e-x) \\n  \\nFigure 3.  (a) Sigmoid activation function equation (b) With HA1 (c) With HA2  \\nùêªùê¥1 =\\n1\\n(1+ùëí‚àíùêª1) =\\n1\\n(1+ùëí‚àí1.2) = 0.6456     (5) \\nùêªùê¥2 =\\n1\\n(1+ùëí‚àíùêª2) =\\n1\\n(1+ùëí‚àí1.5) = 0.9525     (6) \\nHidden layer to Output layer \\nùëÇ1 = (ùêªùê¥1 √ó ùëä5) + (ùêªùê¥2 √ó ùëä7) + (ùêµ2 √ó ùëä0)   (7) \\nùëÇ2 = (ùêªùê¥1 √ó ùëä6) + (ùêªùê¥2 √ó ùëä8) + (ùêµ2 √ó ùëä0)   (8) \\n \\nùëÇ1 = (0.6456 √ó 0.5) + (0.9525 √ó 0.6) + (1 √ó 0.5) = 1.3943  (9) \\nùëÇ1 = (0.6456 √ó 0.7) + (0.9525 √ó 0.8) + (1 √ó 0.5) = 1.71392  (10) \\nApplying activation function for both hidden nodes, here we are using sigmoid activation \\nfunctions.  \\nùëÇùê¥1 =\\n1\\n(1+ùëí‚àíùëÇ1) =\\n1\\n(1+ùëí‚àí1.3943) = 0.8012     (11) \\nùëÇùê¥2 =\\n1\\n(1+ùëí‚àíùëÇ2) =\\n1\\n(1+ùëí‚àí1.7139) = 0.9685     (12) \\n \\nStep 3 \\nIn this step, we need to compute the error value occurred w.r.t. to target output and feed -\\nforward computation values   \\nError= Actual Output ‚Äì Target Output      \\nE1 =OA1  - T1         \\nE2 =OA2  - T2         \\nEtotal = E1+E2                      (13) \\n \\nStep 4 \\nAfter the above operation, need to start backwards step. To update the weights based on \\nthe error value.'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='A Study on Backpropagation in Artificial Neural Networks \\n \\n \\n \\n26 Ch Sekhar and P Sai Meghana \\n \\nFigure 4. Error backpropagated from output to hidden, Hidden to the input layer  \\nHere will compute the what change the error concerning the weight w5  \\nùõøùê∏ùë°ùëúùë°ùëéùëô\\nùõøùë§5 =\\nùõøùê∏ùë°ùëúùë°ùëéùëô\\nùõøùëúùë¢ùë°ùëÇ1 √ó\\nùõøùëúùë¢ùë°ùëÇ1\\nùõøùëõùëíùë°01 √ó\\nùõøùëõùëíùë°01\\nùõøùë§5     (14) \\nWe are spreading in reverse; the first thing we have to do is, compute the adjustment in \\nsimple mistakes w.r.t the yield O1 and O2. New weight is calculated based  \\nùëäùëõùëíùë§ = ùëäùëúùëôùëë + ùëôùëíùëéùëüùëõùëñùëõùëî ùëüùëéùë°ùëí (\\nùõøùê∏ùë°ùëúùë°ùëéùëô\\nùëäùëúùëôùëë\\n)   (15) \\nThe above process explained for one node or perceptron, and it needs to be repeated for all \\nthe nodes and update the weights. With new weights need to calculate the new error aga in \\nuntil getting the error with minimal.  \\n \\n \\nFigure 5. Gaussian function estimation'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\n \\n \\nCopyright ¬© 2020 Global Vision Press (GV Press) 27 \\n5. Applications of back propagation \\nClassification Problems:  Right now, the objective is to distinguish whether a specific \\n‚Äúinformation point ‚Äù has a place with Class 1, 2, or 3. Irregular focuses are allocated to a \\nspecific class, and the neural system is prepared to discover the example. When preparing is \\nfinished, it will utilize what it has figured out how to group new focuses precisely.   \\nFunctional Approximation: Right now, organize attempts to inexact the estimation of a \\nspecific capacity. It is encouraged with full information, and the objective is to locate the \\nactual example. In the wake of preparing, the system effectively gauges the es timation of the \\ngaussian capacity (underneath).   \\nTime-Series Forecasting: Right now, the objective is to structure a neural system to foresee \\na worth dependent on a piece of given time -arrangement information (for example, financial \\nexchange forecast dependent on given patterns). To move toward this issue, the contributions \\nto the neural system must be refactored in lumps, and the subsequent yield will be the \\nfollowing information thing straightforwardly following that piece (see beneath). \\n \\n6. Conclusions \\nIn this paper, we have shown that the process of a backpropagation neural network \\nperforms well on large sets of data. The performance can be improved by changing the \\nnumber of hidden neurons and the learning rate. Because of its iterative training and gradient-\\nbased training, the general speed is far slower than required, so it takes a significant amount \\nof time to train on an extensive set of data. We cannot say that there is a whole network for \\nevery kind of database out there. So keep testing your data on multiple neural networks and \\nsee what fits the best.  \\n \\nReferences \\n[1] Budiharjo S. Triyuni W. Agus Perdana, and H. Tutut, ‚ÄúPredicting tuition fee payment problem using \\nbackpropagation neural network model,‚Äù (2018) \\n[2] M. Huan, C. Ming, and Z. Jianwei, ‚ÄúStudy on  the prediction of real estate price index based on hhga -rbf \\nneural network algorithm ,‚Äù International Journal of u - and e -Service, Science and Technology, SERSC \\nAustralia, ISSN: 2005-4246 (Print); pp.2207-9718 (Online), vol.8, no.7, July, (2015) DOI: 10.142 57/ijunnes \\nst.2015.8.7.11. \\n[3] A. Muhammad, A. Khubaib Amjad , and H. Mehdi, ‚ÄúApplication of data mining using artificial neural \\nnetwork: survey ,‚Äù International Journal of Database Theory and Application, vol.8, no.1, (2015) DOI: \\n10.14257/ijdta.2015.8.1.25. \\n[4] P. Jong, ‚ÄúThe characteristic function of CoreNet (Multi -level single-layer artificial neural network s),‚Äù Asia-\\nPacific Journal of Neura l Networks and Its Applications, vol.1, no.1, (2017) DOI: 10.21742/AJNNIA.201 \\n7.1.1.02 \\n[5] L. Wei, ‚ÄúNeural network model for distortion buckling behaviour of cold-formed steel compression members,‚Äù \\nHelsinki University of Technology Laboratory of Steel Structures Publications 16, (2000) \\n[6] The concept of Back -Propagation Learning by examples from the  http://hebb.cis.uoguelph.ca/~skremer \\n/Teachin g/27642/BP/node3.html')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de01672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = PyPDFLoader('data\\A Study on Backpropagation in Artificial Neural Networks.pdf').load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83fa9411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\nhttp://dx.doi.org/10.21742/ajnnia.2020.4.1.03 \\n \\n \\nPrint ISSN: 2207-8738, eISSN: 2207-8746 AJNNIA \\nCopyright ‚ìí 2020 Global Vision Press (GV Press) \\nA Study on Backpropagation in Artificial Neural Networks \\n \\n \\nCh Sekhar1 and P Sai Meghana2 \\nDepartment of CSE, VIIT(A), AP, India University, India \\n1sekhar1203@gmail.com, 2palavalasasaimeghana@gmail.com \\nAbstract \\nInnovation assumes essential job nowadays in human life to limit the manual work. \\nExecution and exactness with innovation will be high. The Backpropagation neural \\nframework is multilayered, feedforward neural framewo rk and is by a full edge the most \\nextensively utilized. It is moreover seen as one of the least demanding and most wide systems \\nused for managed planning of multilayered neural systems. Backpropagation works by \\napproximating the non -direct association betw een the data and the yield by changing the \\nweight regards inside. It can furthermore be summarized for the data that is rejected from the \\nplanning structures (perceptive limits).  \\n \\nKeywords:  Backpropagation, ANN, Neuron, Nervous system, MLP, Feedforward networks  \\n \\n1. Introduction \\nA neural system is a gathering of associated I/O units where every association has a weight -\\nrelated with its PC programs. It encourages you to develop prescient models from enormous \\ndatabases. This model expands upon the human sen sory system. It encourages you to lead \\npicture understanding, human learning, speech recognition, and so on.  \\nBackpropagation is the embodiment of neural net preparing. It is the technique for \\ntweaking loads of a neural net dependent on the error value got  in the past epoch (i.e., \\nemphasis). Legitimate tuning of the loads permits you to lessen error value and to make the \\nmodel dependable by expanding its speculation. Backpropagation is a compact structure for \\n‚Äúbackward propagation of errors. ‚Äù It is a standa rd technique for preparing artificial neural \\nsystems [1]. This technique assists with computing the inclination of a misfortune work \\nregarding all the loads in the s ystem. Backpropagation recipes from essential standards and \\ngenuine qualities. The neural system uses three information neurons, one shrouded layer with \\ntwo neurons, and a yield layer with two neurons.  \\n \\n2. The literature on back propagation \\nDuring the feed forward computation neural networks, the result output value is not near to \\ntarget or teacher output value. There is a difference between target, and actual feed  forward \\nvalues lead error value. The neural network model tries to give the best predicti on output will \\ngood tolerance for that we have to minimize the error rate. This can be done with \\nBackpropagation \\nMilestone of Backpropagation: \\n                                                           \\nArticle history:  \\nReceived (April 26, 2020), Review Result (May 29, 2020), Accepted (July 10, 2020)'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='A Study on Backpropagation in Artificial Neural Networks \\n \\n \\n \\n22 Ch Sekhar and P Sai Meghana \\n\\uf0b7 In 1961, the essentials idea of ceaseless Backpropagation was inferred with regards to \\ncontrol hypothesis by J. Kelly, Henry Arthur, and E. Bryson.   \\n\\uf0b7 In 1969, Bryson and Ho gave a multi -orchestrate dynamic system improvement \\nmethod.  \\n\\uf0b7 In 1982, Hopfield brought his idea of a neural framework.  \\n\\uf0b7 In 1986, by the effort of David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams, \\nBackpropagation got affirmation.   \\n\\uf0b7 In 1993, Wan was the primary individual to win a stellar example acknowledgement \\nchallenge with the assistance of the backpropagation strategy.  \\n \\n3. Backpropagation algorithm and computational process \\n \\n \\n \\nFigure 1. Flow diagram of the working mechanism of backpropagation \\nThe above [Figure 1] shows that the everyday workflows of the backpropagation process \\nmechanism.  During the backwards propagation, these computation activities will happen as \\nmentioned below. \\n\\uf0b7 Find Error rate: Here we need to calculate the model output with actual output \\n\\uf0b7 Minimum Error: Cross verifying whether the error is minimized or not. \\n\\uf0b7 Update the Weights: The error is more than the acceptable range then, update the \\nweights and biases. After that, again check the error. Repeat the process until the \\nerror becomes low. \\n\\uf0b7 Neural Network Model:  Once the error rate was acceptable range then the model is \\nready to use for forecasting the data \\nThe generalized workflow and stepwise computation in Backpropagation g iven as pseudo-\\ncode as follows: \\nNeural Network \\nModel \\nFind Error rate \\nErr is \\nMin? \\n Update the Weights \\nNN Model is ready \\nWeights'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\n \\n \\nCopyright ¬© 2020 Global Vision Press (GV Press) 23 \\n \\n \\n \\nFigure 2. Feed forward ANN'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='A Study on Backpropagation in Artificial Neural Networks \\n \\n \\n \\n24 Ch Sekhar and P Sai Meghana \\nThe above [Figure 2 ] shows the feedforward artificial neural network contains Input, \\nHidden and Output layers. Each layer contains two nodes with respective weights. All nodes \\nare fully connected model, including the bias node.  \\nNotions of the above network as follows: \\n\\uf09f X1, X2: Input Nodes \\n\\uf09f H1, H2: Hidden Layer Nodes with net out from respective inputs \\n\\uf09f HA1, HA2: Hidden Layer Nodes with activation output  \\n\\uf09f O1, O2: Output Layer Nodes with net out from respective inputs \\n\\uf09f OA1, OA2: Output Layer Nodes with activation output \\n\\uf09f W1 to W8: Weights of respective layers from input to output \\n\\uf09f B1, B2: Bias Nodes for Hidden and Output layers respectively \\n \\n4. Working with backpropagation  \\nThe Following steps are followed involved during Backpropagation of above feedforwa rd \\nnetwork.  \\n \\nStep 1 \\nThe input and target values for this problem are  X1=1, X2=2, I and target values t1 =0.5  \\nand t2=0.05. The weights of the network need to be randomly chosen within the range of 0 to \\n1.  Here we initialize weights as shown in the figure above for understanding the process.  \\n \\nStep 2 \\nFrom the Step1, we got the inputs and respective weights, as it was a feed-forward network. \\nThe neuron will send to next neuron, i.e. hidden layer neuron. As it was fully connected \\nnetwork, each node/neuron will receive inputs from all the nodes/neurons of the input layer.  \\nNow here we are going to calculate the summation output of at each node of the hidden \\nlayer as follows,  \\nùêª1 = (ùëä1 √ó ùëã1) + (ùëä3 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (1) \\nùêª2 = (ùëä2 √ó ùëã1) + (ùëä4 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (2) \\nFrom the above equations, we are going to calculate feed -forward computation from input \\nto hidden and hidden to output layers respectively   \\nInput to the hidden layer \\nùêª1 = (1 √ó 0.1) + (2 √ó 0.3) + (1 √ó 0.5) = 1.2    (3) \\nùêª2 = (1 √ó 0.2) + (2 √ó 0.4) + (1 √ó 0.5) = 1.5    (4) \\nApplying activation function for both hidden nodes, here we are using sigmoid activation \\nfunctions.'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\n \\n \\nCopyright ¬© 2020 Global Vision Press (GV Press) 25 \\nS= 1/ (1+e-x) \\n  \\nFigure 3.  (a) Sigmoid activation function equation (b) With HA1 (c) With HA2  \\nùêªùê¥1 =\\n1\\n(1+ùëí‚àíùêª1) =\\n1\\n(1+ùëí‚àí1.2) = 0.6456     (5) \\nùêªùê¥2 =\\n1\\n(1+ùëí‚àíùêª2) =\\n1\\n(1+ùëí‚àí1.5) = 0.9525     (6) \\nHidden layer to Output layer \\nùëÇ1 = (ùêªùê¥1 √ó ùëä5) + (ùêªùê¥2 √ó ùëä7) + (ùêµ2 √ó ùëä0)   (7) \\nùëÇ2 = (ùêªùê¥1 √ó ùëä6) + (ùêªùê¥2 √ó ùëä8) + (ùêµ2 √ó ùëä0)   (8) \\n \\nùëÇ1 = (0.6456 √ó 0.5) + (0.9525 √ó 0.6) + (1 √ó 0.5) = 1.3943  (9) \\nùëÇ1 = (0.6456 √ó 0.7) + (0.9525 √ó 0.8) + (1 √ó 0.5) = 1.71392  (10) \\nApplying activation function for both hidden nodes, here we are using sigmoid activation \\nfunctions.  \\nùëÇùê¥1 =\\n1\\n(1+ùëí‚àíùëÇ1) =\\n1\\n(1+ùëí‚àí1.3943) = 0.8012     (11) \\nùëÇùê¥2 =\\n1\\n(1+ùëí‚àíùëÇ2) =\\n1\\n(1+ùëí‚àí1.7139) = 0.9685     (12) \\n \\nStep 3 \\nIn this step, we need to compute the error value occurred w.r.t. to target output and feed -\\nforward computation values   \\nError= Actual Output ‚Äì Target Output      \\nE1 =OA1  - T1         \\nE2 =OA2  - T2         \\nEtotal = E1+E2                      (13) \\n \\nStep 4 \\nAfter the above operation, need to start backwards step. To update the weights based on \\nthe error value.'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='A Study on Backpropagation in Artificial Neural Networks \\n \\n \\n \\n26 Ch Sekhar and P Sai Meghana \\n \\nFigure 4. Error backpropagated from output to hidden, Hidden to the input layer  \\nHere will compute the what change the error concerning the weight w5  \\nùõøùê∏ùë°ùëúùë°ùëéùëô\\nùõøùë§5 =\\nùõøùê∏ùë°ùëúùë°ùëéùëô\\nùõøùëúùë¢ùë°ùëÇ1 √ó\\nùõøùëúùë¢ùë°ùëÇ1\\nùõøùëõùëíùë°01 √ó\\nùõøùëõùëíùë°01\\nùõøùë§5     (14) \\nWe are spreading in reverse; the first thing we have to do is, compute the adjustment in \\nsimple mistakes w.r.t the yield O1 and O2. New weight is calculated based  \\nùëäùëõùëíùë§ = ùëäùëúùëôùëë + ùëôùëíùëéùëüùëõùëñùëõùëî ùëüùëéùë°ùëí (\\nùõøùê∏ùë°ùëúùë°ùëéùëô\\nùëäùëúùëôùëë\\n)   (15) \\nThe above process explained for one node or perceptron, and it needs to be repeated for all \\nthe nodes and update the weights. With new weights need to calculate the new error aga in \\nuntil getting the error with minimal.  \\n \\n \\nFigure 5. Gaussian function estimation'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\n \\n \\nCopyright ¬© 2020 Global Vision Press (GV Press) 27 \\n5. Applications of back propagation \\nClassification Problems:  Right now, the objective is to distinguish whether a specific \\n‚Äúinformation point ‚Äù has a place with Class 1, 2, or 3. Irregular focuses are allocated to a \\nspecific class, and the neural system is prepared to discover the example. When preparing is \\nfinished, it will utilize what it has figured out how to group new focuses precisely.   \\nFunctional Approximation: Right now, organize attempts to inexact the estimation of a \\nspecific capacity. It is encouraged with full information, and the objective is to locate the \\nactual example. In the wake of preparing, the system effectively gauges the es timation of the \\ngaussian capacity (underneath).   \\nTime-Series Forecasting: Right now, the objective is to structure a neural system to foresee \\na worth dependent on a piece of given time -arrangement information (for example, financial \\nexchange forecast dependent on given patterns). To move toward this issue, the contributions \\nto the neural system must be refactored in lumps, and the subsequent yield will be the \\nfollowing information thing straightforwardly following that piece (see beneath). \\n \\n6. Conclusions \\nIn this paper, we have shown that the process of a backpropagation neural network \\nperforms well on large sets of data. The performance can be improved by changing the \\nnumber of hidden neurons and the learning rate. Because of its iterative training and gradient-\\nbased training, the general speed is far slower than required, so it takes a significant amount \\nof time to train on an extensive set of data. We cannot say that there is a whole network for \\nevery kind of database out there. So keep testing your data on multiple neural networks and \\nsee what fits the best.  \\n \\nReferences \\n[1] Budiharjo S. Triyuni W. Agus Perdana, and H. Tutut, ‚ÄúPredicting tuition fee payment problem using \\nbackpropagation neural network model,‚Äù (2018) \\n[2] M. Huan, C. Ming, and Z. Jianwei, ‚ÄúStudy on  the prediction of real estate price index based on hhga -rbf \\nneural network algorithm ,‚Äù International Journal of u - and e -Service, Science and Technology, SERSC \\nAustralia, ISSN: 2005-4246 (Print); pp.2207-9718 (Online), vol.8, no.7, July, (2015) DOI: 10.142 57/ijunnes \\nst.2015.8.7.11. \\n[3] A. Muhammad, A. Khubaib Amjad , and H. Mehdi, ‚ÄúApplication of data mining using artificial neural \\nnetwork: survey ,‚Äù International Journal of Database Theory and Application, vol.8, no.1, (2015) DOI: \\n10.14257/ijdta.2015.8.1.25. \\n[4] P. Jong, ‚ÄúThe characteristic function of CoreNet (Multi -level single-layer artificial neural network s),‚Äù Asia-\\nPacific Journal of Neura l Networks and Its Applications, vol.1, no.1, (2017) DOI: 10.21742/AJNNIA.201 \\n7.1.1.02 \\n[5] L. Wei, ‚ÄúNeural network model for distortion buckling behaviour of cold-formed steel compression members,‚Äù \\nHelsinki University of Technology Laboratory of Steel Structures Publications 16, (2000) \\n[6] The concept of Back -Propagation Learning by examples from the  http://hebb.cis.uoguelph.ca/~skremer \\n/Teachin g/27642/BP/node3.html')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae0c640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c933b61",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981452a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempalate = \"write a concise and short summary of the following {text}\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=['text'], template=tempalate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92b2e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='write a concise and short summary of the following {text}')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f771e8",
   "metadata": {},
   "source": [
    "### Chain LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "516684a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7386aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(api_key=GROQ_API_KEY, model='gemma2-9b-it', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac040d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm,\n",
    "                             chain_type= 'stuff', \n",
    "                             prompt=prompt,\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d1f21",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ad352d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_11732\\51517354.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary = chain.run(docs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mwrite a concise and short summary of the following Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      "http://dx.doi.org/10.21742/ajnnia.2020.4.1.03 \n",
      " \n",
      " \n",
      "Print ISSN: 2207-8738, eISSN: 2207-8746 AJNNIA \n",
      "Copyright ‚ìí 2020 Global Vision Press (GV Press) \n",
      "A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      "Ch Sekhar1 and P Sai Meghana2 \n",
      "Department of CSE, VIIT(A), AP, India University, India \n",
      "1sekhar1203@gmail.com, 2palavalasasaimeghana@gmail.com \n",
      "Abstract \n",
      "Innovation assumes essential job nowadays in human life to limit the manual work. \n",
      "Execution and exactness with innovation will be high. The Backpropagation neural \n",
      "framework is multilayered, feedforward neural framewo rk and is by a full edge the most \n",
      "extensively utilized. It is moreover seen as one of the least demanding and most wide systems \n",
      "used for managed planning of multilayered neural systems. Backpropagation works by \n",
      "approximating the non -direct association betw een the data and the yield by changing the \n",
      "weight regards inside. It can furthermore be summarized for the data that is rejected from the \n",
      "planning structures (perceptive limits).  \n",
      " \n",
      "Keywords:  Backpropagation, ANN, Neuron, Nervous system, MLP, Feedforward networks  \n",
      " \n",
      "1. Introduction \n",
      "A neural system is a gathering of associated I/O units where every association has a weight -\n",
      "related with its PC programs. It encourages you to develop prescient models from enormous \n",
      "databases. This model expands upon the human sen sory system. It encourages you to lead \n",
      "picture understanding, human learning, speech recognition, and so on.  \n",
      "Backpropagation is the embodiment of neural net preparing. It is the technique for \n",
      "tweaking loads of a neural net dependent on the error value got  in the past epoch (i.e., \n",
      "emphasis). Legitimate tuning of the loads permits you to lessen error value and to make the \n",
      "model dependable by expanding its speculation. Backpropagation is a compact structure for \n",
      "‚Äúbackward propagation of errors. ‚Äù It is a standa rd technique for preparing artificial neural \n",
      "systems [1]. This technique assists with computing the inclination of a misfortune work \n",
      "regarding all the loads in the s ystem. Backpropagation recipes from essential standards and \n",
      "genuine qualities. The neural system uses three information neurons, one shrouded layer with \n",
      "two neurons, and a yield layer with two neurons.  \n",
      " \n",
      "2. The literature on back propagation \n",
      "During the feed forward computation neural networks, the result output value is not near to \n",
      "target or teacher output value. There is a difference between target, and actual feed  forward \n",
      "values lead error value. The neural network model tries to give the best predicti on output will \n",
      "good tolerance for that we have to minimize the error rate. This can be done with \n",
      "Backpropagation \n",
      "Milestone of Backpropagation: \n",
      "                                                           \n",
      "Article history:  \n",
      "Received (April 26, 2020), Review Result (May 29, 2020), Accepted (July 10, 2020)\n",
      "\n",
      "A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      " \n",
      "22 Ch Sekhar and P Sai Meghana \n",
      "ÔÇ∑ In 1961, the essentials idea of ceaseless Backpropagation was inferred with regards to \n",
      "control hypothesis by J. Kelly, Henry Arthur, and E. Bryson.   \n",
      "ÔÇ∑ In 1969, Bryson and Ho gave a multi -orchestrate dynamic system improvement \n",
      "method.  \n",
      "ÔÇ∑ In 1982, Hopfield brought his idea of a neural framework.  \n",
      "ÔÇ∑ In 1986, by the effort of David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams, \n",
      "Backpropagation got affirmation.   \n",
      "ÔÇ∑ In 1993, Wan was the primary individual to win a stellar example acknowledgement \n",
      "challenge with the assistance of the backpropagation strategy.  \n",
      " \n",
      "3. Backpropagation algorithm and computational process \n",
      " \n",
      " \n",
      " \n",
      "Figure 1. Flow diagram of the working mechanism of backpropagation \n",
      "The above [Figure 1] shows that the everyday workflows of the backpropagation process \n",
      "mechanism.  During the backwards propagation, these computation activities will happen as \n",
      "mentioned below. \n",
      "ÔÇ∑ Find Error rate: Here we need to calculate the model output with actual output \n",
      "ÔÇ∑ Minimum Error: Cross verifying whether the error is minimized or not. \n",
      "ÔÇ∑ Update the Weights: The error is more than the acceptable range then, update the \n",
      "weights and biases. After that, again check the error. Repeat the process until the \n",
      "error becomes low. \n",
      "ÔÇ∑ Neural Network Model:  Once the error rate was acceptable range then the model is \n",
      "ready to use for forecasting the data \n",
      "The generalized workflow and stepwise computation in Backpropagation g iven as pseudo-\n",
      "code as follows: \n",
      "Neural Network \n",
      "Model \n",
      "Find Error rate \n",
      "Err is \n",
      "Min? \n",
      " Update the Weights \n",
      "NN Model is ready \n",
      "Weights\n",
      "\n",
      "Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      " \n",
      " \n",
      "Copyright ¬© 2020 Global Vision Press (GV Press) 23 \n",
      " \n",
      " \n",
      " \n",
      "Figure 2. Feed forward ANN\n",
      "\n",
      "A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      " \n",
      "24 Ch Sekhar and P Sai Meghana \n",
      "The above [Figure 2 ] shows the feedforward artificial neural network contains Input, \n",
      "Hidden and Output layers. Each layer contains two nodes with respective weights. All nodes \n",
      "are fully connected model, including the bias node.  \n",
      "Notions of the above network as follows: \n",
      "ÔÇü X1, X2: Input Nodes \n",
      "ÔÇü H1, H2: Hidden Layer Nodes with net out from respective inputs \n",
      "ÔÇü HA1, HA2: Hidden Layer Nodes with activation output  \n",
      "ÔÇü O1, O2: Output Layer Nodes with net out from respective inputs \n",
      "ÔÇü OA1, OA2: Output Layer Nodes with activation output \n",
      "ÔÇü W1 to W8: Weights of respective layers from input to output \n",
      "ÔÇü B1, B2: Bias Nodes for Hidden and Output layers respectively \n",
      " \n",
      "4. Working with backpropagation  \n",
      "The Following steps are followed involved during Backpropagation of above feedforwa rd \n",
      "network.  \n",
      " \n",
      "Step 1 \n",
      "The input and target values for this problem are  X1=1, X2=2, I and target values t1 =0.5  \n",
      "and t2=0.05. The weights of the network need to be randomly chosen within the range of 0 to \n",
      "1.  Here we initialize weights as shown in the figure above for understanding the process.  \n",
      " \n",
      "Step 2 \n",
      "From the Step1, we got the inputs and respective weights, as it was a feed-forward network. \n",
      "The neuron will send to next neuron, i.e. hidden layer neuron. As it was fully connected \n",
      "network, each node/neuron will receive inputs from all the nodes/neurons of the input layer.  \n",
      "Now here we are going to calculate the summation output of at each node of the hidden \n",
      "layer as follows,  \n",
      "ùêª1 = (ùëä1 √ó ùëã1) + (ùëä3 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (1) \n",
      "ùêª2 = (ùëä2 √ó ùëã1) + (ùëä4 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (2) \n",
      "From the above equations, we are going to calculate feed -forward computation from input \n",
      "to hidden and hidden to output layers respectively   \n",
      "Input to the hidden layer \n",
      "ùêª1 = (1 √ó 0.1) + (2 √ó 0.3) + (1 √ó 0.5) = 1.2    (3) \n",
      "ùêª2 = (1 √ó 0.2) + (2 √ó 0.4) + (1 √ó 0.5) = 1.5    (4) \n",
      "Applying activation function for both hidden nodes, here we are using sigmoid activation \n",
      "functions.\n",
      "\n",
      "Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      " \n",
      " \n",
      "Copyright ¬© 2020 Global Vision Press (GV Press) 25 \n",
      "S= 1/ (1+e-x) \n",
      "  \n",
      "Figure 3.  (a) Sigmoid activation function equation (b) With HA1 (c) With HA2  \n",
      "ùêªùê¥1 =\n",
      "1\n",
      "(1+ùëí‚àíùêª1) =\n",
      "1\n",
      "(1+ùëí‚àí1.2) = 0.6456     (5) \n",
      "ùêªùê¥2 =\n",
      "1\n",
      "(1+ùëí‚àíùêª2) =\n",
      "1\n",
      "(1+ùëí‚àí1.5) = 0.9525     (6) \n",
      "Hidden layer to Output layer \n",
      "ùëÇ1 = (ùêªùê¥1 √ó ùëä5) + (ùêªùê¥2 √ó ùëä7) + (ùêµ2 √ó ùëä0)   (7) \n",
      "ùëÇ2 = (ùêªùê¥1 √ó ùëä6) + (ùêªùê¥2 √ó ùëä8) + (ùêµ2 √ó ùëä0)   (8) \n",
      " \n",
      "ùëÇ1 = (0.6456 √ó 0.5) + (0.9525 √ó 0.6) + (1 √ó 0.5) = 1.3943  (9) \n",
      "ùëÇ1 = (0.6456 √ó 0.7) + (0.9525 √ó 0.8) + (1 √ó 0.5) = 1.71392  (10) \n",
      "Applying activation function for both hidden nodes, here we are using sigmoid activation \n",
      "functions.  \n",
      "ùëÇùê¥1 =\n",
      "1\n",
      "(1+ùëí‚àíùëÇ1) =\n",
      "1\n",
      "(1+ùëí‚àí1.3943) = 0.8012     (11) \n",
      "ùëÇùê¥2 =\n",
      "1\n",
      "(1+ùëí‚àíùëÇ2) =\n",
      "1\n",
      "(1+ùëí‚àí1.7139) = 0.9685     (12) \n",
      " \n",
      "Step 3 \n",
      "In this step, we need to compute the error value occurred w.r.t. to target output and feed -\n",
      "forward computation values   \n",
      "Error= Actual Output ‚Äì Target Output      \n",
      "E1 =OA1  - T1         \n",
      "E2 =OA2  - T2         \n",
      "Etotal = E1+E2                      (13) \n",
      " \n",
      "Step 4 \n",
      "After the above operation, need to start backwards step. To update the weights based on \n",
      "the error value.\n",
      "\n",
      "A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      " \n",
      "26 Ch Sekhar and P Sai Meghana \n",
      " \n",
      "Figure 4. Error backpropagated from output to hidden, Hidden to the input layer  \n",
      "Here will compute the what change the error concerning the weight w5  \n",
      "ùõøùê∏ùë°ùëúùë°ùëéùëô\n",
      "ùõøùë§5 =\n",
      "ùõøùê∏ùë°ùëúùë°ùëéùëô\n",
      "ùõøùëúùë¢ùë°ùëÇ1 √ó\n",
      "ùõøùëúùë¢ùë°ùëÇ1\n",
      "ùõøùëõùëíùë°01 √ó\n",
      "ùõøùëõùëíùë°01\n",
      "ùõøùë§5     (14) \n",
      "We are spreading in reverse; the first thing we have to do is, compute the adjustment in \n",
      "simple mistakes w.r.t the yield O1 and O2. New weight is calculated based  \n",
      "ùëäùëõùëíùë§ = ùëäùëúùëôùëë + ùëôùëíùëéùëüùëõùëñùëõùëî ùëüùëéùë°ùëí (\n",
      "ùõøùê∏ùë°ùëúùë°ùëéùëô\n",
      "ùëäùëúùëôùëë\n",
      ")   (15) \n",
      "The above process explained for one node or perceptron, and it needs to be repeated for all \n",
      "the nodes and update the weights. With new weights need to calculate the new error aga in \n",
      "until getting the error with minimal.  \n",
      " \n",
      " \n",
      "Figure 5. Gaussian function estimation\n",
      "\n",
      "Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      " \n",
      " \n",
      "Copyright ¬© 2020 Global Vision Press (GV Press) 27 \n",
      "5. Applications of back propagation \n",
      "Classification Problems:  Right now, the objective is to distinguish whether a specific \n",
      "‚Äúinformation point ‚Äù has a place with Class 1, 2, or 3. Irregular focuses are allocated to a \n",
      "specific class, and the neural system is prepared to discover the example. When preparing is \n",
      "finished, it will utilize what it has figured out how to group new focuses precisely.   \n",
      "Functional Approximation: Right now, organize attempts to inexact the estimation of a \n",
      "specific capacity. It is encouraged with full information, and the objective is to locate the \n",
      "actual example. In the wake of preparing, the system effectively gauges the es timation of the \n",
      "gaussian capacity (underneath).   \n",
      "Time-Series Forecasting: Right now, the objective is to structure a neural system to foresee \n",
      "a worth dependent on a piece of given time -arrangement information (for example, financial \n",
      "exchange forecast dependent on given patterns). To move toward this issue, the contributions \n",
      "to the neural system must be refactored in lumps, and the subsequent yield will be the \n",
      "following information thing straightforwardly following that piece (see beneath). \n",
      " \n",
      "6. Conclusions \n",
      "In this paper, we have shown that the process of a backpropagation neural network \n",
      "performs well on large sets of data. The performance can be improved by changing the \n",
      "number of hidden neurons and the learning rate. Because of its iterative training and gradient-\n",
      "based training, the general speed is far slower than required, so it takes a significant amount \n",
      "of time to train on an extensive set of data. We cannot say that there is a whole network for \n",
      "every kind of database out there. So keep testing your data on multiple neural networks and \n",
      "see what fits the best.  \n",
      " \n",
      "References \n",
      "[1] Budiharjo S. Triyuni W. Agus Perdana, and H. Tutut, ‚ÄúPredicting tuition fee payment problem using \n",
      "backpropagation neural network model,‚Äù (2018) \n",
      "[2] M. Huan, C. Ming, and Z. Jianwei, ‚ÄúStudy on  the prediction of real estate price index based on hhga -rbf \n",
      "neural network algorithm ,‚Äù International Journal of u - and e -Service, Science and Technology, SERSC \n",
      "Australia, ISSN: 2005-4246 (Print); pp.2207-9718 (Online), vol.8, no.7, July, (2015) DOI: 10.142 57/ijunnes \n",
      "st.2015.8.7.11. \n",
      "[3] A. Muhammad, A. Khubaib Amjad , and H. Mehdi, ‚ÄúApplication of data mining using artificial neural \n",
      "network: survey ,‚Äù International Journal of Database Theory and Application, vol.8, no.1, (2015) DOI: \n",
      "10.14257/ijdta.2015.8.1.25. \n",
      "[4] P. Jong, ‚ÄúThe characteristic function of CoreNet (Multi -level single-layer artificial neural network s),‚Äù Asia-\n",
      "Pacific Journal of Neura l Networks and Its Applications, vol.1, no.1, (2017) DOI: 10.21742/AJNNIA.201 \n",
      "7.1.1.02 \n",
      "[5] L. Wei, ‚ÄúNeural network model for distortion buckling behaviour of cold-formed steel compression members,‚Äù \n",
      "Helsinki University of Technology Laboratory of Steel Structures Publications 16, (2000) \n",
      "[6] The concept of Back -Propagation Learning by examples from the  http://hebb.cis.uoguelph.ca/~skremer \n",
      "/Teachin g/27642/BP/node3.html\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This article provides a concise overview of the backpropagation algorithm used in artificial neural networks (ANNs). \\n\\n**Here's a breakdown of the key points:**\\n\\n* **Introduction:** Backpropagation is a fundamental algorithm for training multi-layered neural networks. It works by adjusting the weights of connections between neurons based on the error between the network's output and the desired output.\\n* **History:** The article briefly traces the history of backpropagation, highlighting key milestones and contributors.\\n* **Algorithm:** The article explains the step-by-step process of backpropagation, including:\\n    * Calculating the error between the network's output and the target output.\\n    * Propagating the error back through the network, layer by layer.\\n    * Adjusting the weights of connections based on the calculated error gradients.\\n* **Applications:** The article mentions several applications of backpropagation, such as:\\n    * **Classification:** Categorizing data into predefined classes.\\n    * **Functional Approximation:** Learning to approximate a function based on given data.\\n    * **Time-Series Forecasting:** Predicting future values in a time series based on past data.\\n* **Conclusion:** The article concludes by emphasizing the effectiveness of backpropagation for training neural networks and acknowledges its limitations, such as the time required for training on large datasets.\\n\\n**Overall, the article provides a clear and concise introduction to the backpropagation algorithm, its history, and its applications.**\\n\\n\\nLet me know if you have any other questions.\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = chain.run(docs)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae112e",
   "metadata": {},
   "source": [
    "# Map Reduce Type\n",
    "- metode summarization yang membutuhkan splitting/ chunking pada documents lalu tiap chunk/potongan document akan dikirimkan ke prompt lalu di summarize (map), lalu tiap tiap summary akan digabungkan dengan prompt untuk menjadi satu kesatuan summary (reduce)\n",
    "- cocok untuk data yang besar (>4000 token, ukuran file >200kb(plain text) dan >500kb(pdf))\n",
    "- lebih lambat dan mahal karena membutuhkan beberapa query ke LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1ae6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792cf7b",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0041d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = PyPDFLoader('data\\A Study on Backpropagation in Artificial Neural Networks.pdf').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77cb2ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\nhttp://dx.doi.org/10.21742/ajnnia.2020.4.1.03 \\n \\n \\nPrint ISSN: 2207-8738, eISSN: 2207-8746 AJNNIA \\nCopyright ‚ìí 2020 Global Vision Press (GV Press) \\nA Study on Backpropagation in Artificial Neural Networks \\n \\n \\nCh Sekhar1 and P Sai Meghana2 \\nDepartment of CSE, VIIT(A), AP, India University, India \\n1sekhar1203@gmail.com, 2palavalasasaimeghana@gmail.com \\nAbstract \\nInnovation assumes essential job nowadays in human life to limit the manual work. \\nExecution and exactness with innovation will be high. The Backpropagation neural \\nframework is multilayered, feedforward neural framewo rk and is by a full edge the most \\nextensively utilized. It is moreover seen as one of the least demanding and most wide systems \\nused for managed planning of multilayered neural systems. Backpropagation works by \\napproximating the non -direct association betw een the data and the yield by changing the \\nweight regards inside. It can furthermore be summarized for the data that is rejected from the \\nplanning structures (perceptive limits).  \\n \\nKeywords:  Backpropagation, ANN, Neuron, Nervous system, MLP, Feedforward networks  \\n \\n1. Introduction \\nA neural system is a gathering of associated I/O units where every association has a weight -\\nrelated with its PC programs. It encourages you to develop prescient models from enormous \\ndatabases. This model expands upon the human sen sory system. It encourages you to lead \\npicture understanding, human learning, speech recognition, and so on.  \\nBackpropagation is the embodiment of neural net preparing. It is the technique for \\ntweaking loads of a neural net dependent on the error value got  in the past epoch (i.e., \\nemphasis). Legitimate tuning of the loads permits you to lessen error value and to make the \\nmodel dependable by expanding its speculation. Backpropagation is a compact structure for \\n‚Äúbackward propagation of errors. ‚Äù It is a standa rd technique for preparing artificial neural \\nsystems [1]. This technique assists with computing the inclination of a misfortune work \\nregarding all the loads in the s ystem. Backpropagation recipes from essential standards and \\ngenuine qualities. The neural system uses three information neurons, one shrouded layer with \\ntwo neurons, and a yield layer with two neurons.  \\n \\n2. The literature on back propagation \\nDuring the feed forward computation neural networks, the result output value is not near to \\ntarget or teacher output value. There is a difference between target, and actual feed  forward \\nvalues lead error value. The neural network model tries to give the best predicti on output will \\ngood tolerance for that we have to minimize the error rate. This can be done with \\nBackpropagation \\nMilestone of Backpropagation: \\n                                                           \\nArticle history:  \\nReceived (April 26, 2020), Review Result (May 29, 2020), Accepted (July 10, 2020)'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='A Study on Backpropagation in Artificial Neural Networks \\n \\n \\n \\n22 Ch Sekhar and P Sai Meghana \\n\\uf0b7 In 1961, the essentials idea of ceaseless Backpropagation was inferred with regards to \\ncontrol hypothesis by J. Kelly, Henry Arthur, and E. Bryson.   \\n\\uf0b7 In 1969, Bryson and Ho gave a multi -orchestrate dynamic system improvement \\nmethod.  \\n\\uf0b7 In 1982, Hopfield brought his idea of a neural framework.  \\n\\uf0b7 In 1986, by the effort of David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams, \\nBackpropagation got affirmation.   \\n\\uf0b7 In 1993, Wan was the primary individual to win a stellar example acknowledgement \\nchallenge with the assistance of the backpropagation strategy.  \\n \\n3. Backpropagation algorithm and computational process \\n \\n \\n \\nFigure 1. Flow diagram of the working mechanism of backpropagation \\nThe above [Figure 1] shows that the everyday workflows of the backpropagation process \\nmechanism.  During the backwards propagation, these computation activities will happen as \\nmentioned below. \\n\\uf0b7 Find Error rate: Here we need to calculate the model output with actual output \\n\\uf0b7 Minimum Error: Cross verifying whether the error is minimized or not. \\n\\uf0b7 Update the Weights: The error is more than the acceptable range then, update the \\nweights and biases. After that, again check the error. Repeat the process until the \\nerror becomes low. \\n\\uf0b7 Neural Network Model:  Once the error rate was acceptable range then the model is \\nready to use for forecasting the data \\nThe generalized workflow and stepwise computation in Backpropagation g iven as pseudo-\\ncode as follows: \\nNeural Network \\nModel \\nFind Error rate \\nErr is \\nMin? \\n Update the Weights \\nNN Model is ready \\nWeights'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\n \\n \\nCopyright ¬© 2020 Global Vision Press (GV Press) 23 \\n \\n \\n \\nFigure 2. Feed forward ANN'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='A Study on Backpropagation in Artificial Neural Networks \\n \\n \\n \\n24 Ch Sekhar and P Sai Meghana \\nThe above [Figure 2 ] shows the feedforward artificial neural network contains Input, \\nHidden and Output layers. Each layer contains two nodes with respective weights. All nodes \\nare fully connected model, including the bias node.  \\nNotions of the above network as follows: \\n\\uf09f X1, X2: Input Nodes \\n\\uf09f H1, H2: Hidden Layer Nodes with net out from respective inputs \\n\\uf09f HA1, HA2: Hidden Layer Nodes with activation output  \\n\\uf09f O1, O2: Output Layer Nodes with net out from respective inputs \\n\\uf09f OA1, OA2: Output Layer Nodes with activation output \\n\\uf09f W1 to W8: Weights of respective layers from input to output \\n\\uf09f B1, B2: Bias Nodes for Hidden and Output layers respectively \\n \\n4. Working with backpropagation  \\nThe Following steps are followed involved during Backpropagation of above feedforwa rd \\nnetwork.  \\n \\nStep 1 \\nThe input and target values for this problem are  X1=1, X2=2, I and target values t1 =0.5  \\nand t2=0.05. The weights of the network need to be randomly chosen within the range of 0 to \\n1.  Here we initialize weights as shown in the figure above for understanding the process.  \\n \\nStep 2 \\nFrom the Step1, we got the inputs and respective weights, as it was a feed-forward network. \\nThe neuron will send to next neuron, i.e. hidden layer neuron. As it was fully connected \\nnetwork, each node/neuron will receive inputs from all the nodes/neurons of the input layer.  \\nNow here we are going to calculate the summation output of at each node of the hidden \\nlayer as follows,  \\nùêª1 = (ùëä1 √ó ùëã1) + (ùëä3 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (1) \\nùêª2 = (ùëä2 √ó ùëã1) + (ùëä4 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (2) \\nFrom the above equations, we are going to calculate feed -forward computation from input \\nto hidden and hidden to output layers respectively   \\nInput to the hidden layer \\nùêª1 = (1 √ó 0.1) + (2 √ó 0.3) + (1 √ó 0.5) = 1.2    (3) \\nùêª2 = (1 √ó 0.2) + (2 √ó 0.4) + (1 √ó 0.5) = 1.5    (4) \\nApplying activation function for both hidden nodes, here we are using sigmoid activation \\nfunctions.'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\n \\n \\nCopyright ¬© 2020 Global Vision Press (GV Press) 25 \\nS= 1/ (1+e-x) \\n  \\nFigure 3.  (a) Sigmoid activation function equation (b) With HA1 (c) With HA2  \\nùêªùê¥1 =\\n1\\n(1+ùëí‚àíùêª1) =\\n1\\n(1+ùëí‚àí1.2) = 0.6456     (5) \\nùêªùê¥2 =\\n1\\n(1+ùëí‚àíùêª2) =\\n1\\n(1+ùëí‚àí1.5) = 0.9525     (6) \\nHidden layer to Output layer \\nùëÇ1 = (ùêªùê¥1 √ó ùëä5) + (ùêªùê¥2 √ó ùëä7) + (ùêµ2 √ó ùëä0)   (7) \\nùëÇ2 = (ùêªùê¥1 √ó ùëä6) + (ùêªùê¥2 √ó ùëä8) + (ùêµ2 √ó ùëä0)   (8) \\n \\nùëÇ1 = (0.6456 √ó 0.5) + (0.9525 √ó 0.6) + (1 √ó 0.5) = 1.3943  (9) \\nùëÇ1 = (0.6456 √ó 0.7) + (0.9525 √ó 0.8) + (1 √ó 0.5) = 1.71392  (10) \\nApplying activation function for both hidden nodes, here we are using sigmoid activation \\nfunctions.  \\nùëÇùê¥1 =\\n1\\n(1+ùëí‚àíùëÇ1) =\\n1\\n(1+ùëí‚àí1.3943) = 0.8012     (11) \\nùëÇùê¥2 =\\n1\\n(1+ùëí‚àíùëÇ2) =\\n1\\n(1+ùëí‚àí1.7139) = 0.9685     (12) \\n \\nStep 3 \\nIn this step, we need to compute the error value occurred w.r.t. to target output and feed -\\nforward computation values   \\nError= Actual Output ‚Äì Target Output      \\nE1 =OA1  - T1         \\nE2 =OA2  - T2         \\nEtotal = E1+E2                      (13) \\n \\nStep 4 \\nAfter the above operation, need to start backwards step. To update the weights based on \\nthe error value.'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='A Study on Backpropagation in Artificial Neural Networks \\n \\n \\n \\n26 Ch Sekhar and P Sai Meghana \\n \\nFigure 4. Error backpropagated from output to hidden, Hidden to the input layer  \\nHere will compute the what change the error concerning the weight w5  \\nùõøùê∏ùë°ùëúùë°ùëéùëô\\nùõøùë§5 =\\nùõøùê∏ùë°ùëúùë°ùëéùëô\\nùõøùëúùë¢ùë°ùëÇ1 √ó\\nùõøùëúùë¢ùë°ùëÇ1\\nùõøùëõùëíùë°01 √ó\\nùõøùëõùëíùë°01\\nùõøùë§5     (14) \\nWe are spreading in reverse; the first thing we have to do is, compute the adjustment in \\nsimple mistakes w.r.t the yield O1 and O2. New weight is calculated based  \\nùëäùëõùëíùë§ = ùëäùëúùëôùëë + ùëôùëíùëéùëüùëõùëñùëõùëî ùëüùëéùë°ùëí (\\nùõøùê∏ùë°ùëúùë°ùëéùëô\\nùëäùëúùëôùëë\\n)   (15) \\nThe above process explained for one node or perceptron, and it needs to be repeated for all \\nthe nodes and update the weights. With new weights need to calculate the new error aga in \\nuntil getting the error with minimal.  \\n \\n \\nFigure 5. Gaussian function estimation'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2013', 'creator': 'Microsoft¬Æ Word 2013', 'creationdate': '2020-09-13T21:23:55+08:00', 'title': 'Journal Paper', 'author': 'GV', 'moddate': '2020-09-13T21:23:55+08:00', 'source': 'data\\\\A Study on Backpropagation in Artificial Neural Networks.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='Asia-Pacific Journal of Neural Networks and Its Applications \\nVol.4, No.1 (2020), pp.21-28 \\n \\n \\nCopyright ¬© 2020 Global Vision Press (GV Press) 27 \\n5. Applications of back propagation \\nClassification Problems:  Right now, the objective is to distinguish whether a specific \\n‚Äúinformation point ‚Äù has a place with Class 1, 2, or 3. Irregular focuses are allocated to a \\nspecific class, and the neural system is prepared to discover the example. When preparing is \\nfinished, it will utilize what it has figured out how to group new focuses precisely.   \\nFunctional Approximation: Right now, organize attempts to inexact the estimation of a \\nspecific capacity. It is encouraged with full information, and the objective is to locate the \\nactual example. In the wake of preparing, the system effectively gauges the es timation of the \\ngaussian capacity (underneath).   \\nTime-Series Forecasting: Right now, the objective is to structure a neural system to foresee \\na worth dependent on a piece of given time -arrangement information (for example, financial \\nexchange forecast dependent on given patterns). To move toward this issue, the contributions \\nto the neural system must be refactored in lumps, and the subsequent yield will be the \\nfollowing information thing straightforwardly following that piece (see beneath). \\n \\n6. Conclusions \\nIn this paper, we have shown that the process of a backpropagation neural network \\nperforms well on large sets of data. The performance can be improved by changing the \\nnumber of hidden neurons and the learning rate. Because of its iterative training and gradient-\\nbased training, the general speed is far slower than required, so it takes a significant amount \\nof time to train on an extensive set of data. We cannot say that there is a whole network for \\nevery kind of database out there. So keep testing your data on multiple neural networks and \\nsee what fits the best.  \\n \\nReferences \\n[1] Budiharjo S. Triyuni W. Agus Perdana, and H. Tutut, ‚ÄúPredicting tuition fee payment problem using \\nbackpropagation neural network model,‚Äù (2018) \\n[2] M. Huan, C. Ming, and Z. Jianwei, ‚ÄúStudy on  the prediction of real estate price index based on hhga -rbf \\nneural network algorithm ,‚Äù International Journal of u - and e -Service, Science and Technology, SERSC \\nAustralia, ISSN: 2005-4246 (Print); pp.2207-9718 (Online), vol.8, no.7, July, (2015) DOI: 10.142 57/ijunnes \\nst.2015.8.7.11. \\n[3] A. Muhammad, A. Khubaib Amjad , and H. Mehdi, ‚ÄúApplication of data mining using artificial neural \\nnetwork: survey ,‚Äù International Journal of Database Theory and Application, vol.8, no.1, (2015) DOI: \\n10.14257/ijdta.2015.8.1.25. \\n[4] P. Jong, ‚ÄúThe characteristic function of CoreNet (Multi -level single-layer artificial neural network s),‚Äù Asia-\\nPacific Journal of Neura l Networks and Its Applications, vol.1, no.1, (2017) DOI: 10.21742/AJNNIA.201 \\n7.1.1.02 \\n[5] L. Wei, ‚ÄúNeural network model for distortion buckling behaviour of cold-formed steel compression members,‚Äù \\nHelsinki University of Technology Laboratory of Steel Structures Publications 16, (2000) \\n[6] The concept of Back -Propagation Learning by examples from the  http://hebb.cis.uoguelph.ca/~skremer \\n/Teachin g/27642/BP/node3.html')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38410804",
   "metadata": {},
   "source": [
    "### Splitting Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81a3253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=500)\n",
    "splitted_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69e050d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740c66d",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0061a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt for each chunk documents\n",
    "map_template = \"\"\"\n",
    "please summarize the documents {text}\n",
    "Summary : \n",
    "\"\"\"\n",
    "\n",
    "map_prompt_template = PromptTemplate(input_variables=['text'],\n",
    "                                     template=map_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e4d1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_template = \"\"\"\n",
    "provide the final summary of the entire documents with these important points. Add a title, start the precise summary with an introduction and provide summary in number points for the speech {text}\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt_template = PromptTemplate(input_variables=['text'], template=reduce_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6e82c",
   "metadata": {},
   "source": [
    "### Chain LLM map-reduce type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77729a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt = map_prompt_template,\n",
    "    combine_prompt = reduce_prompt_template,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4584a",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d667c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "please summarize the documents Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      "http://dx.doi.org/10.21742/ajnnia.2020.4.1.03 \n",
      " \n",
      " \n",
      "Print ISSN: 2207-8738, eISSN: 2207-8746 AJNNIA \n",
      "Copyright ‚ìí 2020 Global Vision Press (GV Press) \n",
      "A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      "Ch Sekhar1 and P Sai Meghana2 \n",
      "Department of CSE, VIIT(A), AP, India University, India \n",
      "1sekhar1203@gmail.com, 2palavalasasaimeghana@gmail.com \n",
      "Abstract \n",
      "Innovation assumes essential job nowadays in human life to limit the manual work. \n",
      "Execution and exactness with innovation will be high. The Backpropagation neural \n",
      "framework is multilayered, feedforward neural framewo rk and is by a full edge the most \n",
      "extensively utilized. It is moreover seen as one of the least demanding and most wide systems \n",
      "used for managed planning of multilayered neural systems. Backpropagation works by \n",
      "approximating the non -direct association betw een the data and the yield by changing the \n",
      "weight regards inside. It can furthermore be summarized for the data that is rejected from the \n",
      "planning structures (perceptive limits).  \n",
      " \n",
      "Keywords:  Backpropagation, ANN, Neuron, Nervous system, MLP, Feedforward networks  \n",
      " \n",
      "1. Introduction \n",
      "A neural system is a gathering of associated I/O units where every association has a weight -\n",
      "related with its PC programs. It encourages you to develop prescient models from enormous \n",
      "databases. This model expands upon the human sen sory system. It encourages you to lead \n",
      "picture understanding, human learning, speech recognition, and so on.  \n",
      "Backpropagation is the embodiment of neural net preparing. It is the technique for \n",
      "tweaking loads of a neural net dependent on the error value got  in the past epoch (i.e., \n",
      "emphasis). Legitimate tuning of the loads permits you to lessen error value and to make the \n",
      "model dependable by expanding its speculation. Backpropagation is a compact structure for\n",
      "Summary : \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "please summarize the documents picture understanding, human learning, speech recognition, and so on.  \n",
      "Backpropagation is the embodiment of neural net preparing. It is the technique for \n",
      "tweaking loads of a neural net dependent on the error value got  in the past epoch (i.e., \n",
      "emphasis). Legitimate tuning of the loads permits you to lessen error value and to make the \n",
      "model dependable by expanding its speculation. Backpropagation is a compact structure for \n",
      "‚Äúbackward propagation of errors. ‚Äù It is a standa rd technique for preparing artificial neural \n",
      "systems [1]. This technique assists with computing the inclination of a misfortune work \n",
      "regarding all the loads in the s ystem. Backpropagation recipes from essential standards and \n",
      "genuine qualities. The neural system uses three information neurons, one shrouded layer with \n",
      "two neurons, and a yield layer with two neurons.  \n",
      " \n",
      "2. The literature on back propagation \n",
      "During the feed forward computation neural networks, the result output value is not near to \n",
      "target or teacher output value. There is a difference between target, and actual feed  forward \n",
      "values lead error value. The neural network model tries to give the best predicti on output will \n",
      "good tolerance for that we have to minimize the error rate. This can be done with \n",
      "Backpropagation \n",
      "Milestone of Backpropagation: \n",
      "                                                           \n",
      "Article history:  \n",
      "Received (April 26, 2020), Review Result (May 29, 2020), Accepted (July 10, 2020)\n",
      "Summary : \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "please summarize the documents A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      " \n",
      "22 Ch Sekhar and P Sai Meghana \n",
      "ÔÇ∑ In 1961, the essentials idea of ceaseless Backpropagation was inferred with regards to \n",
      "control hypothesis by J. Kelly, Henry Arthur, and E. Bryson.   \n",
      "ÔÇ∑ In 1969, Bryson and Ho gave a multi -orchestrate dynamic system improvement \n",
      "method.  \n",
      "ÔÇ∑ In 1982, Hopfield brought his idea of a neural framework.  \n",
      "ÔÇ∑ In 1986, by the effort of David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams, \n",
      "Backpropagation got affirmation.   \n",
      "ÔÇ∑ In 1993, Wan was the primary individual to win a stellar example acknowledgement \n",
      "challenge with the assistance of the backpropagation strategy.  \n",
      " \n",
      "3. Backpropagation algorithm and computational process \n",
      " \n",
      " \n",
      " \n",
      "Figure 1. Flow diagram of the working mechanism of backpropagation \n",
      "The above [Figure 1] shows that the everyday workflows of the backpropagation process \n",
      "mechanism.  During the backwards propagation, these computation activities will happen as \n",
      "mentioned below. \n",
      "ÔÇ∑ Find Error rate: Here we need to calculate the model output with actual output \n",
      "ÔÇ∑ Minimum Error: Cross verifying whether the error is minimized or not. \n",
      "ÔÇ∑ Update the Weights: The error is more than the acceptable range then, update the \n",
      "weights and biases. After that, again check the error. Repeat the process until the \n",
      "error becomes low. \n",
      "ÔÇ∑ Neural Network Model:  Once the error rate was acceptable range then the model is \n",
      "ready to use for forecasting the data \n",
      "The generalized workflow and stepwise computation in Backpropagation g iven as pseudo-\n",
      "code as follows: \n",
      "Neural Network \n",
      "Model \n",
      "Find Error rate \n",
      "Err is \n",
      "Min? \n",
      " Update the Weights \n",
      "NN Model is ready \n",
      "Weights\n",
      "Summary : \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "please summarize the documents Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      " \n",
      " \n",
      "Copyright ¬© 2020 Global Vision Press (GV Press) 23 \n",
      " \n",
      " \n",
      " \n",
      "Figure 2. Feed forward ANN\n",
      "Summary : \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "please summarize the documents A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      " \n",
      "24 Ch Sekhar and P Sai Meghana \n",
      "The above [Figure 2 ] shows the feedforward artificial neural network contains Input, \n",
      "Hidden and Output layers. Each layer contains two nodes with respective weights. All nodes \n",
      "are fully connected model, including the bias node.  \n",
      "Notions of the above network as follows: \n",
      "ÔÇü X1, X2: Input Nodes \n",
      "ÔÇü H1, H2: Hidden Layer Nodes with net out from respective inputs \n",
      "ÔÇü HA1, HA2: Hidden Layer Nodes with activation output  \n",
      "ÔÇü O1, O2: Output Layer Nodes with net out from respective inputs \n",
      "ÔÇü OA1, OA2: Output Layer Nodes with activation output \n",
      "ÔÇü W1 to W8: Weights of respective layers from input to output \n",
      "ÔÇü B1, B2: Bias Nodes for Hidden and Output layers respectively \n",
      " \n",
      "4. Working with backpropagation  \n",
      "The Following steps are followed involved during Backpropagation of above feedforwa rd \n",
      "network.  \n",
      " \n",
      "Step 1 \n",
      "The input and target values for this problem are  X1=1, X2=2, I and target values t1 =0.5  \n",
      "and t2=0.05. The weights of the network need to be randomly chosen within the range of 0 to \n",
      "1.  Here we initialize weights as shown in the figure above for understanding the process.  \n",
      " \n",
      "Step 2 \n",
      "From the Step1, we got the inputs and respective weights, as it was a feed-forward network. \n",
      "The neuron will send to next neuron, i.e. hidden layer neuron. As it was fully connected \n",
      "network, each node/neuron will receive inputs from all the nodes/neurons of the input layer.  \n",
      "Now here we are going to calculate the summation output of at each node of the hidden \n",
      "layer as follows,  \n",
      "ùêª1 = (ùëä1 √ó ùëã1) + (ùëä3 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (1) \n",
      "ùêª2 = (ùëä2 √ó ùëã1) + (ùëä4 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (2) \n",
      "From the above equations, we are going to calculate feed -forward computation from input \n",
      "to hidden and hidden to output layers respectively   \n",
      "Input to the hidden layer \n",
      "ùêª1 = (1 √ó 0.1) + (2 √ó 0.3) + (1 √ó 0.5) = 1.2    (3) \n",
      "ùêª2 = (1 √ó 0.2) + (2 √ó 0.4) + (1 √ó 0.5) = 1.5    (4)\n",
      "Summary : \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "please summarize the documents Now here we are going to calculate the summation output of at each node of the hidden \n",
      "layer as follows,  \n",
      "ùêª1 = (ùëä1 √ó ùëã1) + (ùëä3 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (1) \n",
      "ùêª2 = (ùëä2 √ó ùëã1) + (ùëä4 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (2) \n",
      "From the above equations, we are going to calculate feed -forward computation from input \n",
      "to hidden and hidden to output layers respectively   \n",
      "Input to the hidden layer \n",
      "ùêª1 = (1 √ó 0.1) + (2 √ó 0.3) + (1 √ó 0.5) = 1.2    (3) \n",
      "ùêª2 = (1 √ó 0.2) + (2 √ó 0.4) + (1 √ó 0.5) = 1.5    (4) \n",
      "Applying activation function for both hidden nodes, here we are using sigmoid activation \n",
      "functions.\n",
      "Summary : \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "please summarize the documents Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      " \n",
      " \n",
      "Copyright ¬© 2020 Global Vision Press (GV Press) 25 \n",
      "S= 1/ (1+e-x) \n",
      "  \n",
      "Figure 3.  (a) Sigmoid activation function equation (b) With HA1 (c) With HA2  \n",
      "ùêªùê¥1 =\n",
      "1\n",
      "(1+ùëí‚àíùêª1) =\n",
      "1\n",
      "(1+ùëí‚àí1.2) = 0.6456     (5) \n",
      "ùêªùê¥2 =\n",
      "1\n",
      "(1+ùëí‚àíùêª2) =\n",
      "1\n",
      "(1+ùëí‚àí1.5) = 0.9525     (6) \n",
      "Hidden layer to Output layer \n",
      "ùëÇ1 = (ùêªùê¥1 √ó ùëä5) + (ùêªùê¥2 √ó ùëä7) + (ùêµ2 √ó ùëä0)   (7) \n",
      "ùëÇ2 = (ùêªùê¥1 √ó ùëä6) + (ùêªùê¥2 √ó ùëä8) + (ùêµ2 √ó ùëä0)   (8) \n",
      " \n",
      "ùëÇ1 = (0.6456 √ó 0.5) + (0.9525 √ó 0.6) + (1 √ó 0.5) = 1.3943  (9) \n",
      "ùëÇ1 = (0.6456 √ó 0.7) + (0.9525 √ó 0.8) + (1 √ó 0.5) = 1.71392  (10) \n",
      "Applying activation function for both hidden nodes, here we are using sigmoid activation \n",
      "functions.  \n",
      "ùëÇùê¥1 =\n",
      "1\n",
      "(1+ùëí‚àíùëÇ1) =\n",
      "1\n",
      "(1+ùëí‚àí1.3943) = 0.8012     (11) \n",
      "ùëÇùê¥2 =\n",
      "1\n",
      "(1+ùëí‚àíùëÇ2) =\n",
      "1\n",
      "(1+ùëí‚àí1.7139) = 0.9685     (12) \n",
      " \n",
      "Step 3 \n",
      "In this step, we need to compute the error value occurred w.r.t. to target output and feed -\n",
      "forward computation values   \n",
      "Error= Actual Output ‚Äì Target Output      \n",
      "E1 =OA1  - T1         \n",
      "E2 =OA2  - T2         \n",
      "Etotal = E1+E2                      (13) \n",
      " \n",
      "Step 4 \n",
      "After the above operation, need to start backwards step. To update the weights based on \n",
      "the error value.\n",
      "Summary : \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "please summarize the documents A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      " \n",
      "26 Ch Sekhar and P Sai Meghana \n",
      " \n",
      "Figure 4. Error backpropagated from output to hidden, Hidden to the input layer  \n",
      "Here will compute the what change the error concerning the weight w5  \n",
      "ùõøùê∏ùë°ùëúùë°ùëéùëô\n",
      "ùõøùë§5 =\n",
      "ùõøùê∏ùë°ùëúùë°ùëéùëô\n",
      "ùõøùëúùë¢ùë°ùëÇ1 √ó\n",
      "ùõøùëúùë¢ùë°ùëÇ1\n",
      "ùõøùëõùëíùë°01 √ó\n",
      "ùõøùëõùëíùë°01\n",
      "ùõøùë§5     (14) \n",
      "We are spreading in reverse; the first thing we have to do is, compute the adjustment in \n",
      "simple mistakes w.r.t the yield O1 and O2. New weight is calculated based  \n",
      "ùëäùëõùëíùë§ = ùëäùëúùëôùëë + ùëôùëíùëéùëüùëõùëñùëõùëî ùëüùëéùë°ùëí (\n",
      "ùõøùê∏ùë°ùëúùë°ùëéùëô\n",
      "ùëäùëúùëôùëë\n",
      ")   (15) \n",
      "The above process explained for one node or perceptron, and it needs to be repeated for all \n",
      "the nodes and update the weights. With new weights need to calculate the new error aga in \n",
      "until getting the error with minimal.  \n",
      " \n",
      " \n",
      "Figure 5. Gaussian function estimation\n",
      "Summary : \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "please summarize the documents Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      " \n",
      " \n",
      "Copyright ¬© 2020 Global Vision Press (GV Press) 27 \n",
      "5. Applications of back propagation \n",
      "Classification Problems:  Right now, the objective is to distinguish whether a specific \n",
      "‚Äúinformation point ‚Äù has a place with Class 1, 2, or 3. Irregular focuses are allocated to a \n",
      "specific class, and the neural system is prepared to discover the example. When preparing is \n",
      "finished, it will utilize what it has figured out how to group new focuses precisely.   \n",
      "Functional Approximation: Right now, organize attempts to inexact the estimation of a \n",
      "specific capacity. It is encouraged with full information, and the objective is to locate the \n",
      "actual example. In the wake of preparing, the system effectively gauges the es timation of the \n",
      "gaussian capacity (underneath).   \n",
      "Time-Series Forecasting: Right now, the objective is to structure a neural system to foresee \n",
      "a worth dependent on a piece of given time -arrangement information (for example, financial \n",
      "exchange forecast dependent on given patterns). To move toward this issue, the contributions \n",
      "to the neural system must be refactored in lumps, and the subsequent yield will be the \n",
      "following information thing straightforwardly following that piece (see beneath). \n",
      " \n",
      "6. Conclusions \n",
      "In this paper, we have shown that the process of a backpropagation neural network \n",
      "performs well on large sets of data. The performance can be improved by changing the \n",
      "number of hidden neurons and the learning rate. Because of its iterative training and gradient-\n",
      "based training, the general speed is far slower than required, so it takes a significant amount \n",
      "of time to train on an extensive set of data. We cannot say that there is a whole network for \n",
      "every kind of database out there. So keep testing your data on multiple neural networks and \n",
      "see what fits the best.  \n",
      " \n",
      "References\n",
      "Summary : \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "please summarize the documents number of hidden neurons and the learning rate. Because of its iterative training and gradient-\n",
      "based training, the general speed is far slower than required, so it takes a significant amount \n",
      "of time to train on an extensive set of data. We cannot say that there is a whole network for \n",
      "every kind of database out there. So keep testing your data on multiple neural networks and \n",
      "see what fits the best.  \n",
      " \n",
      "References \n",
      "[1] Budiharjo S. Triyuni W. Agus Perdana, and H. Tutut, ‚ÄúPredicting tuition fee payment problem using \n",
      "backpropagation neural network model,‚Äù (2018) \n",
      "[2] M. Huan, C. Ming, and Z. Jianwei, ‚ÄúStudy on  the prediction of real estate price index based on hhga -rbf \n",
      "neural network algorithm ,‚Äù International Journal of u - and e -Service, Science and Technology, SERSC \n",
      "Australia, ISSN: 2005-4246 (Print); pp.2207-9718 (Online), vol.8, no.7, July, (2015) DOI: 10.142 57/ijunnes \n",
      "st.2015.8.7.11. \n",
      "[3] A. Muhammad, A. Khubaib Amjad , and H. Mehdi, ‚ÄúApplication of data mining using artificial neural \n",
      "network: survey ,‚Äù International Journal of Database Theory and Application, vol.8, no.1, (2015) DOI: \n",
      "10.14257/ijdta.2015.8.1.25. \n",
      "[4] P. Jong, ‚ÄúThe characteristic function of CoreNet (Multi -level single-layer artificial neural network s),‚Äù Asia-\n",
      "Pacific Journal of Neura l Networks and Its Applications, vol.1, no.1, (2017) DOI: 10.21742/AJNNIA.201 \n",
      "7.1.1.02 \n",
      "[5] L. Wei, ‚ÄúNeural network model for distortion buckling behaviour of cold-formed steel compression members,‚Äù \n",
      "Helsinki University of Technology Laboratory of Steel Structures Publications 16, (2000) \n",
      "[6] The concept of Back -Propagation Learning by examples from the  http://hebb.cis.uoguelph.ca/~skremer \n",
      "/Teachin g/27642/BP/node3.html\n",
      "Summary : \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python Learning\\PYTHON-UDEMY\\langchain_project\\text_summarization\\.myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3183 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "provide the final summary of the entire documents with these important points. Add a title, start the precise summary with an introduction and provide summary in number points for the speech This article provides an overview of the backpropagation algorithm, a fundamental technique used to train artificial neural networks (ANNs). \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Backpropagation's Importance:** Backpropagation is crucial for training multilayered feedforward neural networks (MLP), which are widely used in various applications like image recognition, speech processing, and prediction.\n",
      "* **How it Works:** Backpropagation adjusts the weights of connections between neurons in the network based on the difference between the network's output and the desired output (the error). This iterative process aims to minimize the error and improve the network's accuracy.\n",
      "* **Advantages:** Backpropagation is considered a relatively simple and effective method for training ANNs, especially for complex, non-linear relationships between input and output data.\n",
      "\n",
      "**The article also:**\n",
      "\n",
      "* Briefly explains the concept of artificial neural networks and their inspiration from the human nervous system.\n",
      "* Highlights the role of weights in determining the strength of connections between neurons.\n",
      "* Mentions the importance of training data in enabling the network to learn and make accurate predictions.\n",
      "\n",
      "\n",
      "Overall, the article serves as a concise introduction to backpropagation, emphasizing its significance in the field of artificial intelligence and machine learning. \n",
      "\n",
      "\n",
      "The provided text focuses on **backpropagation**, a fundamental algorithm used to train artificial neural networks. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "**What is Backpropagation?**\n",
      "\n",
      "* It's the core method for adjusting the \"weights\" (parameters) of a neural network.\n",
      "* Weights are tweaked based on the difference between the network's predicted output and the desired (target) output. This difference is called the \"error.\"\n",
      "* By iteratively adjusting weights, backpropagation aims to minimize the error and improve the network's accuracy.\n",
      "\n",
      "**How it Works:**\n",
      "\n",
      "1. **Feedforward:** The input data flows through the network, layer by layer, until it reaches the output layer.\n",
      "2. **Error Calculation:** The output is compared to the target value, and the error is calculated.\n",
      "3. **Backward Propagation:** The error is propagated back through the network, layer by layer.\n",
      "4. **Weight Adjustment:**  At each layer, the weights are adjusted slightly to reduce the error. This adjustment is based on the calculated gradients (derivatives) of the error function with respect to each weight.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* Backpropagation is a \"gradient descent\" algorithm, meaning it iteratively moves the weights in the direction that minimizes the error.\n",
      "* It relies on the chain rule of calculus to calculate the gradients.\n",
      "* The text mentions a simple example network with three input neurons, two hidden neurons, and two output neurons.\n",
      "\n",
      "**Importance:**\n",
      "\n",
      "* Backpropagation is a cornerstone of deep learning, enabling the training of complex neural networks with many layers.\n",
      "* It has revolutionized fields like image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about backpropagation or neural networks!\n",
      "\n",
      "\n",
      "This document provides a brief history and overview of the backpropagation algorithm in artificial neural networks. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **History:**\n",
      "    * The foundational ideas of backpropagation were explored in control theory in the 1960s.\n",
      "    *  Hopfield introduced the concept of neural networks in 1982.\n",
      "    *  Rumelhart, Hinton, and Williams popularized backpropagation in 1986.\n",
      "    *  Wan achieved a breakthrough in 1993 by winning a pattern recognition challenge using backpropagation.\n",
      "\n",
      "* **Algorithm:**\n",
      "    * Backpropagation is an iterative algorithm used to train artificial neural networks.\n",
      "    * It works by calculating the error between the network's output and the desired output, then adjusting the network's weights to minimize this error.\n",
      "    * The process involves:\n",
      "        1. **Calculating the error rate:** Comparing the network's output to the actual output.\n",
      "        2. **Checking for minimum error:** Determining if the error is sufficiently low.\n",
      "        3. **Updating weights:** If the error is too high, adjusting the weights and biases of the network to improve its performance.\n",
      "        4. **Repeating steps 1-3:** Continuing to iterate until the error rate is acceptable.\n",
      "\n",
      "* **Workflow:**\n",
      "    * The document provides a flow diagram illustrating the steps involved in backpropagation.\n",
      "    * It also presents pseudo-code summarizing the algorithm's general workflow.\n",
      "\n",
      "\n",
      "\n",
      "Essentially, the document highlights the historical development and fundamental workings of backpropagation, a crucial algorithm for training artificial neural networks.\n",
      "\n",
      "\n",
      "Please provide me with the content of the document. I need the text of the article \"Feed Forward Artificial Neural Networks: A Review\" from the Asia-Pacific Journal of Neural Networks and Its Applications Vol.4, No.1 (2020), pp.21-28 to be able to summarize it for you. \n",
      "\n",
      "Once you provide the text, I can analyze it and generate a concise and informative summary. \n",
      "\n",
      "\n",
      "\n",
      "The document \"A Study on Backpropagation in Artificial Neural Networks\" by Ch Sekhar and P Sai Meghana describes the basic principles of backpropagation in a simple feedforward neural network. \n",
      "\n",
      "Here's a summary:\n",
      "\n",
      "**Network Structure:**\n",
      "\n",
      "* The network consists of an input layer (X1, X2), a hidden layer (H1, H2), and an output layer (O1, O2).\n",
      "* Each layer has two nodes, fully connected to each other.\n",
      "* Weights (W1 to W8) connect the nodes, and bias nodes (B1, B2) provide an additional input to the hidden and output layers.\n",
      "\n",
      "**Backpropagation Process:**\n",
      "\n",
      "1. **Initialization:** The document starts with example input values (X1=1, X2=2) and target values (t1=0.5, t2=0.05).  Weights are randomly initialized between 0 and 1.\n",
      "\n",
      "2. **Feedforward:**  Input values are propagated through the network. The hidden layer nodes calculate their outputs (H1, H2) based on weighted sums of inputs and biases. These outputs are then used by the output layer nodes to calculate their outputs (O1, O2).\n",
      "\n",
      "3. **Backpropagation (not detailed in the excerpt):**  The document mentions the backpropagation process but doesn't provide the detailed steps. Backpropagation involves calculating the error between the network's output and the target values, and then adjusting the weights based on this error. This process is typically done iteratively, gradually improving the network's accuracy.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "* **Feedforward:**  Information flows in one direction, from input to output.\n",
      "* **Backpropagation:**  Error signals are propagated back through the network to adjust weights and improve performance.\n",
      "* **Weights:**  Parameters that determine the strength of connections between neurons.\n",
      "* **Activation Function:**  Not explicitly mentioned in the excerpt, but activation functions introduce non-linearity into the network, allowing it to learn complex patterns.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "\n",
      "\n",
      "The document describes the calculation of the output of each node in a hidden layer of a neural network. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Equations:** Two equations (1 & 2) define the calculation of the output (H1 and H2) for each hidden node, based on the weighted sum of inputs (X1 and X2) and a bias term (B1).\n",
      "* **Feed-Forward Computation:** The equations represent the feed-forward computation from the input layer to the hidden layer.\n",
      "* **Input to Hidden Layer:** Specific input values (X1 and X2) and weights (W1, W2, W3, W4) are used to calculate the output of each hidden node (H1 and H2) using the defined equations (3 & 4).\n",
      "* **Activation Function:** The sigmoid activation function is applied to the output of each hidden node to introduce non-linearity.\n",
      "\n",
      "\n",
      "In essence, the document outlines a basic example of how a neural network processes information through its hidden layers using weighted sums and activation functions. \n",
      "\n",
      "\n",
      "This document describes a simplified example of a feed-forward neural network with one hidden layer, using the sigmoid activation function. \n",
      "\n",
      "**Here's a breakdown:**\n",
      "\n",
      "1. **Network Structure:** The network has two inputs, two hidden nodes, and two outputs.\n",
      "\n",
      "2. **Forward Propagation:**\n",
      "    * The input values are multiplied by the corresponding weights and summed, then passed through the sigmoid activation function to produce the hidden layer outputs (HA1 and HA2).\n",
      "    * The hidden layer outputs are again multiplied by weights and summed, along with a bias term, to produce the output layer values (O1 and O2).\n",
      "    * These output values are then passed through the sigmoid activation function to get the final network outputs (OA1 and OA2).\n",
      "\n",
      "3. **Error Calculation:**\n",
      "    * The difference between the actual output (OA1 and OA2) and the target output (T1 and T2) is calculated to determine the error.\n",
      "\n",
      "4. **Backpropagation:**\n",
      "    * This step is not fully explained in the provided summary, but it involves calculating the gradients of the error with respect to the weights and biases. \n",
      "    * These gradients are then used to update the weights and biases in a way that minimizes the error.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Sigmoid Activation Function:** This function squashes the input values to a range between 0 and 1, introducing non-linearity into the network.\n",
      "* **Weights and Biases:** These parameters determine the strength of the connections between neurons and are adjusted during training to improve the network's performance.\n",
      "* **Backpropagation:** This algorithm is crucial for training neural networks as it allows the network to learn from its errors and adjust its weights accordingly.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "\n",
      "\n",
      "The document \"A Study on Backpropagation in Artificial Neural Networks\" by Ch Sekhar and P Sai Meghana focuses on explaining the backpropagation algorithm, a key method for training artificial neural networks. \n",
      "\n",
      "Here's a breakdown of the key points:\n",
      "\n",
      "* **Backpropagation:** The algorithm works by iteratively adjusting the weights of the network based on the error between the network's output and the desired output. This adjustment is done by propagating the error backwards through the network, layer by layer.\n",
      "\n",
      "* **Error Calculation:** The document explains how the error is calculated for each node in the network, using the chain rule of calculus. It illustrates this with a specific example, focusing on the weight `w5` and its impact on the output `O1`.\n",
      "\n",
      "* **Weight Update:** The document describes the formula for updating the weights based on the calculated error. The new weight is calculated by adding a fraction of the error to the old weight, multiplied by a learning rate.\n",
      "\n",
      "* **Iterative Process:** The process of calculating the error and updating the weights is repeated iteratively until the error reaches a minimum. This means the network continuously learns and improves its performance.\n",
      "\n",
      "* **Gaussian Function Estimation:** The document mentions the use of a Gaussian function for activation in the network. This function introduces non-linearity, allowing the network to learn complex patterns.\n",
      "\n",
      "**In essence, the document provides a clear and concise explanation of the backpropagation algorithm, highlighting its core concepts and steps involved in training a neural network.**\n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "provide the final summary of the entire documents with these important points. Add a title, start the precise summary with an introduction and provide summary in number points for the speech This paper explores the applications of backpropagation neural networks, a type of artificial neural network widely used in machine learning. \n",
      "\n",
      "**Key takeaways:**\n",
      "\n",
      "* **Backpropagation's versatility:** The paper highlights three main applications of backpropagation:\n",
      "    * **Classification:**  Categorizing data points into predefined classes (e.g., spam detection).\n",
      "    * **Functional Approximation:**  Learning to approximate a function's behavior based on input-output pairs (e.g., predicting stock prices).\n",
      "    * **Time-Series Forecasting:** Predicting future values in a sequence based on past data (e.g., weather forecasting).\n",
      "\n",
      "* **Performance and optimization:** The paper emphasizes that backpropagation's performance can be improved by:\n",
      "    * **Tuning the number of hidden neurons:**  More neurons can increase complexity and potentially improve accuracy, but also risk overfitting.\n",
      "    * **Adjusting the learning rate:** This controls the step size during training and affects the speed and stability of learning.\n",
      "\n",
      "* **Training time:** The paper acknowledges that backpropagation can be computationally intensive, especially for large datasets, due to its iterative training process.\n",
      "\n",
      "* **No one-size-fits-all:** The paper stresses the importance of experimenting with different network architectures and hyperparameters to find the best solution for a specific dataset.\n",
      "\n",
      "\n",
      "**Overall, the paper provides a concise overview of backpropagation's capabilities and limitations, emphasizing its adaptability to various machine learning tasks while acknowledging the need for careful tuning and consideration of training time.**\n",
      "\n",
      "\n",
      "The provided text discusses the training process of neural networks but doesn't specify the number of hidden neurons or learning rate for any particular network. \n",
      "\n",
      "Here's a breakdown of the key points:\n",
      "\n",
      "* **Training Time:** Neural networks are known for their iterative training process, which can be slow, especially with large datasets.\n",
      "* **No One-Size-Fits-All:** There isn't a single neural network architecture that works best for every type of database. \n",
      "* **Experimentation:** The text emphasizes the need to test different neural network configurations (likely including variations in hidden neurons and learning rates) to find the optimal model for a specific dataset.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions. \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "provide the final summary of the entire documents with these important points. Add a title, start the precise summary with an introduction and provide summary in number points for the speech You've provided a great overview of the backpropagation algorithm and its role in training artificial neural networks.  \n",
      "\n",
      "Here's a consolidated summary incorporating the key points from all the documents you've shared:\n",
      "\n",
      "**Backpropagation: The Engine of Neural Network Learning**\n",
      "\n",
      "Backpropagation is the fundamental algorithm that powers the training of artificial neural networks (ANNs), enabling them to learn complex patterns and make accurate predictions. \n",
      "\n",
      "**How it Works:**\n",
      "\n",
      "1. **Feedforward:** Input data flows through the network, layer by layer, from input nodes to hidden nodes and finally to output nodes.\n",
      "\n",
      "2. **Error Calculation:** The output of the network is compared to the desired (target) output, and the difference (error) is calculated.\n",
      "\n",
      "3. **Backward Propagation:** The error is propagated back through the network, layer by layer.\n",
      "\n",
      "4. **Weight Adjustment:** At each layer, the weights connecting the neurons are adjusted slightly to reduce the error. This adjustment is based on the calculated gradients (derivatives) of the error function with respect to each weight.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "* **Weights:**  Parameters that determine the strength of connections between neurons. They are adjusted during training to minimize the error.\n",
      "* **Activation Function:** Introduces non-linearity into the network, allowing it to learn complex relationships. Common activation functions include sigmoid, ReLU, and tanh.\n",
      "* **Learning Rate:**  A parameter that controls the size of the weight adjustments. A smaller learning rate leads to slower but potentially more stable learning.\n",
      "* **Gradient Descent:** Backpropagation is a form of gradient descent, an iterative optimization algorithm that moves the weights in the direction that minimizes the error.\n",
      "\n",
      "**Importance:**\n",
      "\n",
      "* **Foundation of Deep Learning:** Backpropagation is essential for training deep neural networks, which have multiple hidden layers and can learn highly complex representations.\n",
      "* **Wide Applications:** It has revolutionized fields like image recognition, natural language processing, speech recognition, and machine translation.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "* **Vanishing/Exploding Gradients:**  Can occur in deep networks, making it difficult to train effectively. Techniques like batch normalization and residual connections help mitigate this issue.\n",
      "* **Local Minima:** The error surface can have multiple local minima, and backpropagation may get stuck in a suboptimal solution.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions or would like to explore specific aspects of backpropagation in more detail!\n",
      "\n",
      "\n",
      "## Backpropagation: A Versatile Tool for Machine Learning\n",
      "\n",
      "This paper delves into the applications and considerations surrounding backpropagation neural networks, a cornerstone of modern machine learning.  \n",
      "\n",
      "Here are the key takeaways:\n",
      "\n",
      "1. **Diverse Applications:** Backpropagation excels in a variety of machine learning tasks, including:\n",
      "    * **Classification:** Accurately sorting data into predefined categories (e.g., identifying spam emails).\n",
      "    * **Functional Approximation:** Learning to mimic the behavior of a function based on input-output examples (e.g., predicting stock market trends).\n",
      "    * **Time-Series Forecasting:** Predicting future values in a sequence based on historical data (e.g., forecasting weather patterns).\n",
      "\n",
      "2. **Performance Optimization:**  Achieving optimal performance with backpropagation involves:\n",
      "    * **Careful Neuron Tuning:** The number of hidden neurons in the network can significantly impact accuracy. Finding the right balance between complexity and overfitting is crucial.\n",
      "    * **Learning Rate Adjustment:** This parameter controls the step size during training.  Finding the optimal learning rate is essential for both speed and stability of learning.\n",
      "\n",
      "3. **Computational Demands:**  Backpropagation can be computationally expensive, especially when dealing with large datasets. Its iterative training process requires significant processing power.\n",
      "\n",
      "4. **No Universal Solution:**  There is no single \"best\" backpropagation network architecture or set of hyperparameters.  Success depends on carefully experimenting with different configurations to find the optimal solution for a specific dataset and task.\n",
      "\n",
      "\n",
      "\n",
      "In conclusion, backpropagation offers a powerful and versatile framework for machine learning, but its effectiveness hinges on careful tuning, consideration of computational resources, and a willingness to experiment. \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "## Backpropagation: The Powerhouse of Neural Network Learning\n",
      "\n",
      "This document provides a comprehensive overview of backpropagation, the fundamental algorithm driving the training of artificial neural networks (ANNs).  \n",
      "\n",
      "**Here's a summary of the key points:**\n",
      "\n",
      "1. **Core Function:** Backpropagation enables ANNs to learn complex patterns and make accurate predictions by adjusting the weights connecting neurons based on the difference between predicted and desired outputs (error).\n",
      "\n",
      "2. **Training Process:**\n",
      "    * **Feedforward:** Input data flows through the network, layer by layer, from input to output nodes.\n",
      "    * **Error Calculation:** The difference between the network's output and the target output is calculated.\n",
      "    * **Backward Propagation:** The error is propagated back through the network, layer by layer.\n",
      "    * **Weight Adjustment:** Weights are adjusted slightly at each layer to minimize the error, guided by calculated gradients.\n",
      "\n",
      "3. **Key Concepts:**\n",
      "    * **Weights:** Parameters determining the strength of connections between neurons; adjusted during training.\n",
      "    * **Activation Function:** Introduces non-linearity, allowing the network to learn complex relationships.\n",
      "    * **Learning Rate:** Controls the size of weight adjustments; a smaller rate leads to slower but potentially more stable learning.\n",
      "    * **Gradient Descent:** Backpropagation utilizes gradient descent to iteratively minimize the error.\n",
      "\n",
      "4. **Impact and Applications:**\n",
      "    * **Foundation of Deep Learning:** Essential for training deep neural networks with multiple hidden layers.\n",
      "    * **Wide-Ranging Applications:** Revolutionized fields like image recognition, natural language processing, speech recognition, and machine translation.\n",
      "\n",
      "5. **Challenges:**\n",
      "    * **Vanishing/Exploding Gradients:** Can occur in deep networks, hindering effective training. Techniques like batch normalization and residual connections help mitigate this.\n",
      "    * **Local Minima:** The error surface can have multiple local minima, potentially trapping backpropagation in suboptimal solutions.\n",
      "\n",
      "\n",
      "Backpropagation remains a cornerstone of machine learning, enabling the development of powerful AI systems. Understanding its principles and challenges is crucial for effectively leveraging its potential. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run llm with splitted documents\n",
    "summary = summary_chain.run(splitted_docs)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14b7b8",
   "metadata": {},
   "source": [
    "### Refine Chain Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df1005",
   "metadata": {},
   "source": [
    "- Menggunakan hasil summary dari chunk pertama dan chunk document ke prompt dan llm untuk summarization \n",
    "- hasil summary dari chunk document sebelumnya di terus ke prompt untuk document selanjutnya \n",
    "- summary diperbaiki secara bertahap dengan menambahkan bagian bagian(chunk) selanjutnya\n",
    "\n",
    "- keuntungan : lebih kontekstual, karena menggunakan summary sebelumnya\n",
    "- boros token karena panjang token terus nambah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4cc225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "337cb4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      "http://dx.doi.org/10.21742/ajnnia.2020.4.1.03 \n",
      " \n",
      " \n",
      "Print ISSN: 2207-8738, eISSN: 2207-8746 AJNNIA \n",
      "Copyright ‚ìí 2020 Global Vision Press (GV Press) \n",
      "A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      "Ch Sekhar1 and P Sai Meghana2 \n",
      "Department of CSE, VIIT(A), AP, India University, India \n",
      "1sekhar1203@gmail.com, 2palavalasasaimeghana@gmail.com \n",
      "Abstract \n",
      "Innovation assumes essential job nowadays in human life to limit the manual work. \n",
      "Execution and exactness with innovation will be high. The Backpropagation neural \n",
      "framework is multilayered, feedforward neural framewo rk and is by a full edge the most \n",
      "extensively utilized. It is moreover seen as one of the least demanding and most wide systems \n",
      "used for managed planning of multilayered neural systems. Backpropagation works by \n",
      "approximating the non -direct association betw een the data and the yield by changing the \n",
      "weight regards inside. It can furthermore be summarized for the data that is rejected from the \n",
      "planning structures (perceptive limits).  \n",
      " \n",
      "Keywords:  Backpropagation, ANN, Neuron, Nervous system, MLP, Feedforward networks  \n",
      " \n",
      "1. Introduction \n",
      "A neural system is a gathering of associated I/O units where every association has a weight -\n",
      "related with its PC programs. It encourages you to develop prescient models from enormous \n",
      "databases. This model expands upon the human sen sory system. It encourages you to lead \n",
      "picture understanding, human learning, speech recognition, and so on.  \n",
      "Backpropagation is the embodiment of neural net preparing. It is the technique for \n",
      "tweaking loads of a neural net dependent on the error value got  in the past epoch (i.e., \n",
      "emphasis). Legitimate tuning of the loads permits you to lessen error value and to make the \n",
      "model dependable by expanding its speculation. Backpropagation is a compact structure for\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper explores Backpropagation, a fundamental algorithm used to train artificial neural networks (ANNs). \n",
      "\n",
      "Backpropagation is a multilayered, feedforward neural network training method widely used for supervised learning. It works by adjusting the weights within the network to minimize the difference between predicted and actual outputs. This process involves calculating the error at each layer and propagating it back through the network to adjust the weights accordingly. \n",
      "\n",
      "The paper highlights Backpropagation's importance in enabling ANNs to learn complex patterns and relationships from data, leading to applications in various fields like image recognition, speech processing, and prediction modeling. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "picture understanding, human learning, speech recognition, and so on.  \n",
      "Backpropagation is the embodiment of neural net preparing. It is the technique for \n",
      "tweaking loads of a neural net dependent on the error value got  in the past epoch (i.e., \n",
      "emphasis). Legitimate tuning of the loads permits you to lessen error value and to make the \n",
      "model dependable by expanding its speculation. Backpropagation is a compact structure for \n",
      "‚Äúbackward propagation of errors. ‚Äù It is a standa rd technique for preparing artificial neural \n",
      "systems [1]. This technique assists with computing the inclination of a misfortune work \n",
      "regarding all the loads in the s ystem. Backpropagation recipes from essential standards and \n",
      "genuine qualities. The neural system uses three information neurons, one shrouded layer with \n",
      "two neurons, and a yield layer with two neurons.  \n",
      " \n",
      "2. The literature on back propagation \n",
      "During the feed forward computation neural networks, the result output value is not near to \n",
      "target or teacher output value. There is a difference between target, and actual feed  forward \n",
      "values lead error value. The neural network model tries to give the best predicti on output will \n",
      "good tolerance for that we have to minimize the error rate. This can be done with \n",
      "Backpropagation \n",
      "Milestone of Backpropagation: \n",
      "                                                           \n",
      "Article history:  \n",
      "Received (April 26, 2020), Review Result (May 29, 2020), Accepted (July 10, 2020)\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper explores Backpropagation, a fundamental algorithm used to train artificial neural networks (ANNs). \n",
      "\n",
      "Backpropagation is a multilayered, feedforward neural network training method widely used for supervised learning. It works by adjusting the weights within the network to minimize the difference between predicted and actual outputs. This process involves calculating the error at each layer and propagating it back through the network to adjust the weights accordingly.  Backpropagation is essentially the method for tweaking the weights of a neural network based on the error value calculated in the previous epoch.  This fine-tuning of weights allows for a reduction in the error value and improves the model's reliability by enhancing its predictions.  \n",
      "\n",
      "The paper highlights Backpropagation's importance in enabling ANNs to learn complex patterns and relationships from data, leading to applications in various fields like image recognition, speech processing, prediction modeling, and more. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      " \n",
      "22 Ch Sekhar and P Sai Meghana \n",
      "ÔÇ∑ In 1961, the essentials idea of ceaseless Backpropagation was inferred with regards to \n",
      "control hypothesis by J. Kelly, Henry Arthur, and E. Bryson.   \n",
      "ÔÇ∑ In 1969, Bryson and Ho gave a multi -orchestrate dynamic system improvement \n",
      "method.  \n",
      "ÔÇ∑ In 1982, Hopfield brought his idea of a neural framework.  \n",
      "ÔÇ∑ In 1986, by the effort of David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams, \n",
      "Backpropagation got affirmation.   \n",
      "ÔÇ∑ In 1993, Wan was the primary individual to win a stellar example acknowledgement \n",
      "challenge with the assistance of the backpropagation strategy.  \n",
      " \n",
      "3. Backpropagation algorithm and computational process \n",
      " \n",
      " \n",
      " \n",
      "Figure 1. Flow diagram of the working mechanism of backpropagation \n",
      "The above [Figure 1] shows that the everyday workflows of the backpropagation process \n",
      "mechanism.  During the backwards propagation, these computation activities will happen as \n",
      "mentioned below. \n",
      "ÔÇ∑ Find Error rate: Here we need to calculate the model output with actual output \n",
      "ÔÇ∑ Minimum Error: Cross verifying whether the error is minimized or not. \n",
      "ÔÇ∑ Update the Weights: The error is more than the acceptable range then, update the \n",
      "weights and biases. After that, again check the error. Repeat the process until the \n",
      "error becomes low. \n",
      "ÔÇ∑ Neural Network Model:  Once the error rate was acceptable range then the model is \n",
      "ready to use for forecasting the data \n",
      "The generalized workflow and stepwise computation in Backpropagation g iven as pseudo-\n",
      "code as follows: \n",
      "Neural Network \n",
      "Model \n",
      "Find Error rate \n",
      "Err is \n",
      "Min? \n",
      " Update the Weights \n",
      "NN Model is ready \n",
      "Weights\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper explores Backpropagation, a fundamental algorithm used to train artificial neural networks (ANNs). \n",
      "\n",
      "Backpropagation is a multilayered, feedforward neural network training method widely used for supervised learning. It works by adjusting the weights within the network to minimize the difference between predicted and actual outputs. This process involves calculating the error at each layer and propagating it back through the network to adjust the weights accordingly.  Backpropagation is essentially the method for tweaking the weights of a neural network based on the error value calculated in the previous epoch.  This fine-tuning of weights allows for a reduction in the error value and improves the model's reliability by enhancing its predictions.  \n",
      "\n",
      "The history of Backpropagation dates back to the 1960s with early concepts in control theory, but it gained prominence in the 1980s with the work of Rumelhart, Hinton, and Williams, who solidified its application in neural networks.  By the 1990s, Backpropagation had proven its effectiveness, with Wan achieving a breakthrough in pattern recognition using the algorithm.\n",
      "\n",
      "Backpropagation's importance lies in its ability to enable ANNs to learn complex patterns and relationships from data, leading to applications in various fields like image recognition, speech processing, prediction modeling, and more. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      " \n",
      " \n",
      "Copyright ¬© 2020 Global Vision Press (GV Press) 23 \n",
      " \n",
      " \n",
      " \n",
      "Figure 2. Feed forward ANN\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper explores Backpropagation, a fundamental algorithm used to train artificial neural networks (ANNs). \n",
      "\n",
      "Backpropagation is a multilayered, feedforward neural network training method widely used for supervised learning. It works by adjusting the weights within the network to minimize the difference between predicted and actual outputs. This process involves calculating the error at each layer and propagating it back through the network to adjust the weights accordingly.  Backpropagation is essentially the method for tweaking the weights of a neural network based on the error value calculated in the previous epoch.  This fine-tuning of weights allows for a reduction in the error value and improves the model's reliability by enhancing its predictions.  \n",
      "\n",
      "The history of Backpropagation dates back to the 1960s with early concepts in control theory, but it gained prominence in the 1980s with the work of Rumelhart, Hinton, and Williams, who solidified its application in neural networks.  By the 1990s, Backpropagation had proven its effectiveness, with Wan achieving a breakthrough in pattern recognition using the algorithm.\n",
      "\n",
      "Backpropagation's importance lies in its ability to enable ANNs to learn complex patterns and relationships from data, leading to applications in various fields like image recognition, speech processing, prediction modeling, and more. \n",
      "\n",
      "\n",
      "**The provided figure and journal information are not directly relevant to refining the existing summary.** \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      " \n",
      "24 Ch Sekhar and P Sai Meghana \n",
      "The above [Figure 2 ] shows the feedforward artificial neural network contains Input, \n",
      "Hidden and Output layers. Each layer contains two nodes with respective weights. All nodes \n",
      "are fully connected model, including the bias node.  \n",
      "Notions of the above network as follows: \n",
      "ÔÇü X1, X2: Input Nodes \n",
      "ÔÇü H1, H2: Hidden Layer Nodes with net out from respective inputs \n",
      "ÔÇü HA1, HA2: Hidden Layer Nodes with activation output  \n",
      "ÔÇü O1, O2: Output Layer Nodes with net out from respective inputs \n",
      "ÔÇü OA1, OA2: Output Layer Nodes with activation output \n",
      "ÔÇü W1 to W8: Weights of respective layers from input to output \n",
      "ÔÇü B1, B2: Bias Nodes for Hidden and Output layers respectively \n",
      " \n",
      "4. Working with backpropagation  \n",
      "The Following steps are followed involved during Backpropagation of above feedforwa rd \n",
      "network.  \n",
      " \n",
      "Step 1 \n",
      "The input and target values for this problem are  X1=1, X2=2, I and target values t1 =0.5  \n",
      "and t2=0.05. The weights of the network need to be randomly chosen within the range of 0 to \n",
      "1.  Here we initialize weights as shown in the figure above for understanding the process.  \n",
      " \n",
      "Step 2 \n",
      "From the Step1, we got the inputs and respective weights, as it was a feed-forward network. \n",
      "The neuron will send to next neuron, i.e. hidden layer neuron. As it was fully connected \n",
      "network, each node/neuron will receive inputs from all the nodes/neurons of the input layer.  \n",
      "Now here we are going to calculate the summation output of at each node of the hidden \n",
      "layer as follows,  \n",
      "ùêª1 = (ùëä1 √ó ùëã1) + (ùëä3 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (1) \n",
      "ùêª2 = (ùëä2 √ó ùëã1) + (ùëä4 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (2) \n",
      "From the above equations, we are going to calculate feed -forward computation from input \n",
      "to hidden and hidden to output layers respectively   \n",
      "Input to the hidden layer \n",
      "ùêª1 = (1 √ó 0.1) + (2 √ó 0.3) + (1 √ó 0.5) = 1.2    (3) \n",
      "ùêª2 = (1 √ó 0.2) + (2 √ó 0.4) + (1 √ó 0.5) = 1.5    (4)\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper explores Backpropagation, a fundamental algorithm used to train artificial neural networks (ANNs). \n",
      "\n",
      "Backpropagation is a multilayered, feedforward neural network training method widely used for supervised learning. It works by adjusting the weights within the network to minimize the difference between predicted and actual outputs. This process involves calculating the error at each layer and propagating it back through the network to adjust the weights accordingly.  Backpropagation is essentially the method for tweaking the weights of a neural network based on the error value calculated in the previous epoch.  This fine-tuning of weights allows for a reduction in the error value and improves the model's reliability by enhancing its predictions.  \n",
      "\n",
      "The history of Backpropagation dates back to the 1960s with early concepts in control theory, but it gained prominence in the 1980s with the work of Rumelhart, Hinton, and Williams, who solidified its application in neural networks.  By the 1990s, Backpropagation had proven its effectiveness, with Wan achieving a breakthrough in pattern recognition using the algorithm.\n",
      "\n",
      "Backpropagation's importance lies in its ability to enable ANNs to learn complex patterns and relationships from data, leading to applications in various fields like image recognition, speech processing, prediction modeling, and more.  The algorithm works by iteratively adjusting the weights of connections between neurons in a network based on the difference between the network's output and the desired output. This process, often visualized as \"feeding forward\" input through the network and then \"backpropagating\" the error signal to adjust weights, allows the network to learn and improve its performance over time. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Now here we are going to calculate the summation output of at each node of the hidden \n",
      "layer as follows,  \n",
      "ùêª1 = (ùëä1 √ó ùëã1) + (ùëä3 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (1) \n",
      "ùêª2 = (ùëä2 √ó ùëã1) + (ùëä4 √ó ùëã2) + (ùêµ1 √ó ùëä0)    (2) \n",
      "From the above equations, we are going to calculate feed -forward computation from input \n",
      "to hidden and hidden to output layers respectively   \n",
      "Input to the hidden layer \n",
      "ùêª1 = (1 √ó 0.1) + (2 √ó 0.3) + (1 √ó 0.5) = 1.2    (3) \n",
      "ùêª2 = (1 √ó 0.2) + (2 √ó 0.4) + (1 √ó 0.5) = 1.5    (4) \n",
      "Applying activation function for both hidden nodes, here we are using sigmoid activation \n",
      "functions.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper explores Backpropagation, a fundamental algorithm used to train artificial neural networks (ANNs). \n",
      "\n",
      "Backpropagation is a multilayered, feedforward neural network training method widely used for supervised learning. It works by adjusting the weights within the network to minimize the difference between predicted and actual outputs. This process involves calculating the error at each layer and propagating it back through the network to adjust the weights accordingly.  Backpropagation is essentially the method for tweaking the weights of a neural network based on the error value calculated in the previous epoch.  This fine-tuning of weights allows for a reduction in the error value and improves the model's reliability by enhancing its predictions.  \n",
      "\n",
      "The history of Backpropagation dates back to the 1960s with early concepts in control theory, but it gained prominence in the 1980s with the work of Rumelhart, Hinton, and Williams, who solidified its application in neural networks.  By the 1990s, Backpropagation had proven its effectiveness, with Wan achieving a breakthrough in pattern recognition using the algorithm.\n",
      "\n",
      "Backpropagation's importance lies in its ability to enable ANNs to learn complex patterns and relationships from data, leading to applications in various fields like image recognition, speech processing, prediction modeling, and more.  The algorithm works by iteratively adjusting the weights of connections between neurons in a network based on the difference between the network's output and the desired output. This process, often visualized as \"feeding forward\" input through the network and then \"backpropagating\" the error signal to adjust weights, allows the network to learn and improve its performance over time.  \n",
      "\n",
      "The provided context delves into a specific example of how backpropagation works, illustrating the calculation of hidden layer outputs using weighted sums and activation functions. \n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      " \n",
      " \n",
      "Copyright ¬© 2020 Global Vision Press (GV Press) 25 \n",
      "S= 1/ (1+e-x) \n",
      "  \n",
      "Figure 3.  (a) Sigmoid activation function equation (b) With HA1 (c) With HA2  \n",
      "ùêªùê¥1 =\n",
      "1\n",
      "(1+ùëí‚àíùêª1) =\n",
      "1\n",
      "(1+ùëí‚àí1.2) = 0.6456     (5) \n",
      "ùêªùê¥2 =\n",
      "1\n",
      "(1+ùëí‚àíùêª2) =\n",
      "1\n",
      "(1+ùëí‚àí1.5) = 0.9525     (6) \n",
      "Hidden layer to Output layer \n",
      "ùëÇ1 = (ùêªùê¥1 √ó ùëä5) + (ùêªùê¥2 √ó ùëä7) + (ùêµ2 √ó ùëä0)   (7) \n",
      "ùëÇ2 = (ùêªùê¥1 √ó ùëä6) + (ùêªùê¥2 √ó ùëä8) + (ùêµ2 √ó ùëä0)   (8) \n",
      " \n",
      "ùëÇ1 = (0.6456 √ó 0.5) + (0.9525 √ó 0.6) + (1 √ó 0.5) = 1.3943  (9) \n",
      "ùëÇ1 = (0.6456 √ó 0.7) + (0.9525 √ó 0.8) + (1 √ó 0.5) = 1.71392  (10) \n",
      "Applying activation function for both hidden nodes, here we are using sigmoid activation \n",
      "functions.  \n",
      "ùëÇùê¥1 =\n",
      "1\n",
      "(1+ùëí‚àíùëÇ1) =\n",
      "1\n",
      "(1+ùëí‚àí1.3943) = 0.8012     (11) \n",
      "ùëÇùê¥2 =\n",
      "1\n",
      "(1+ùëí‚àíùëÇ2) =\n",
      "1\n",
      "(1+ùëí‚àí1.7139) = 0.9685     (12) \n",
      " \n",
      "Step 3 \n",
      "In this step, we need to compute the error value occurred w.r.t. to target output and feed -\n",
      "forward computation values   \n",
      "Error= Actual Output ‚Äì Target Output      \n",
      "E1 =OA1  - T1         \n",
      "E2 =OA2  - T2         \n",
      "Etotal = E1+E2                      (13) \n",
      " \n",
      "Step 4 \n",
      "After the above operation, need to start backwards step. To update the weights based on \n",
      "the error value.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper explores Backpropagation, a fundamental algorithm used to train artificial neural networks (ANNs). \n",
      "\n",
      "Backpropagation is a multilayered, feedforward neural network training method widely used for supervised learning. It works by adjusting the weights within the network to minimize the difference between predicted and actual outputs. This process involves calculating the error at each layer and propagating it back through the network to adjust the weights accordingly.  Backpropagation is essentially the method for tweaking the weights of a neural network based on the error value calculated in the previous epoch.  This fine-tuning of weights allows for a reduction in the error value and improves the model's reliability by enhancing its predictions.  \n",
      "\n",
      "The history of Backpropagation dates back to the 1960s with early concepts in control theory, but it gained prominence in the 1980s with the work of Rumelhart, Hinton, and Williams, who solidified its application in neural networks.  By the 1990s, Backpropagation had proven its effectiveness, with Wan achieving a breakthrough in pattern recognition using the algorithm.\n",
      "\n",
      "Backpropagation's importance lies in its ability to enable ANNs to learn complex patterns and relationships from data, leading to applications in various fields like image recognition, speech processing, prediction modeling, and more.  The algorithm works by iteratively adjusting the weights of connections between neurons in a network based on the difference between the network's output and the desired output. This process, often visualized as \"feeding forward\" input through the network and then \"backpropagating\" the error signal to adjust weights, allows the network to learn and improve its performance over time.  \n",
      "\n",
      "The provided context further illustrates this process by demonstrating how backpropagation calculates hidden layer outputs using weighted sums and activation functions (like sigmoid). It then outlines the steps involved in calculating the error between the network's output and the target output, and how this error is used to update the weights in a backward pass. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "A Study on Backpropagation in Artificial Neural Networks \n",
      " \n",
      " \n",
      " \n",
      "26 Ch Sekhar and P Sai Meghana \n",
      " \n",
      "Figure 4. Error backpropagated from output to hidden, Hidden to the input layer  \n",
      "Here will compute the what change the error concerning the weight w5  \n",
      "ùõøùê∏ùë°ùëúùë°ùëéùëô\n",
      "ùõøùë§5 =\n",
      "ùõøùê∏ùë°ùëúùë°ùëéùëô\n",
      "ùõøùëúùë¢ùë°ùëÇ1 √ó\n",
      "ùõøùëúùë¢ùë°ùëÇ1\n",
      "ùõøùëõùëíùë°01 √ó\n",
      "ùõøùëõùëíùë°01\n",
      "ùõøùë§5     (14) \n",
      "We are spreading in reverse; the first thing we have to do is, compute the adjustment in \n",
      "simple mistakes w.r.t the yield O1 and O2. New weight is calculated based  \n",
      "ùëäùëõùëíùë§ = ùëäùëúùëôùëë + ùëôùëíùëéùëüùëõùëñùëõùëî ùëüùëéùë°ùëí (\n",
      "ùõøùê∏ùë°ùëúùë°ùëéùëô\n",
      "ùëäùëúùëôùëë\n",
      ")   (15) \n",
      "The above process explained for one node or perceptron, and it needs to be repeated for all \n",
      "the nodes and update the weights. With new weights need to calculate the new error aga in \n",
      "until getting the error with minimal.  \n",
      " \n",
      " \n",
      "Figure 5. Gaussian function estimation\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper explores Backpropagation, a fundamental algorithm used to train artificial neural networks (ANNs). \n",
      "\n",
      "Backpropagation is a multilayered, feedforward neural network training method widely used for supervised learning. It works by adjusting the weights within the network to minimize the difference between predicted and actual outputs. This process involves calculating the error at each layer and propagating it back through the network to adjust the weights accordingly.  Backpropagation is essentially the method for tweaking the weights of a neural network based on the error value calculated in the previous epoch.  This fine-tuning of weights allows for a reduction in the error value and improves the model's reliability by enhancing its predictions.  \n",
      "\n",
      "The history of Backpropagation dates back to the 1960s with early concepts in control theory, but it gained prominence in the 1980s with the work of Rumelhart, Hinton, and Williams, who solidified its application in neural networks.  By the 1990s, Backpropagation had proven its effectiveness, with Wan achieving a breakthrough in pattern recognition using the algorithm.\n",
      "\n",
      "Backpropagation's importance lies in its ability to enable ANNs to learn complex patterns and relationships from data, leading to applications in various fields like image recognition, speech processing, prediction modeling, and more.  The algorithm works by iteratively adjusting the weights of connections between neurons in a network based on the difference between the network's output and the desired output. This process, often visualized as \"feeding forward\" input through the network and then \"backpropagating\" the error signal to adjust weights, allows the network to learn and improve its performance over time.  \n",
      "\n",
      "The provided context further illustrates this process by demonstrating how backpropagation calculates hidden layer outputs using weighted sums and activation functions (like sigmoid). It then outlines the steps involved in calculating the error between the network's output and the target output, and how this error is used to update the weights in a backward pass. The context also provides a formula (equation 14) for calculating the change in error with respect to a weight (ùõøùê∏ùë°ùëúùë°ùëéùëô\n",
      "ùõøùë§5) and a formula (equation 15) for updating the weight based on the learning rate and the error change. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Asia-Pacific Journal of Neural Networks and Its Applications \n",
      "Vol.4, No.1 (2020), pp.21-28 \n",
      " \n",
      " \n",
      "Copyright ¬© 2020 Global Vision Press (GV Press) 27 \n",
      "5. Applications of back propagation \n",
      "Classification Problems:  Right now, the objective is to distinguish whether a specific \n",
      "‚Äúinformation point ‚Äù has a place with Class 1, 2, or 3. Irregular focuses are allocated to a \n",
      "specific class, and the neural system is prepared to discover the example. When preparing is \n",
      "finished, it will utilize what it has figured out how to group new focuses precisely.   \n",
      "Functional Approximation: Right now, organize attempts to inexact the estimation of a \n",
      "specific capacity. It is encouraged with full information, and the objective is to locate the \n",
      "actual example. In the wake of preparing, the system effectively gauges the es timation of the \n",
      "gaussian capacity (underneath).   \n",
      "Time-Series Forecasting: Right now, the objective is to structure a neural system to foresee \n",
      "a worth dependent on a piece of given time -arrangement information (for example, financial \n",
      "exchange forecast dependent on given patterns). To move toward this issue, the contributions \n",
      "to the neural system must be refactored in lumps, and the subsequent yield will be the \n",
      "following information thing straightforwardly following that piece (see beneath). \n",
      " \n",
      "6. Conclusions \n",
      "In this paper, we have shown that the process of a backpropagation neural network \n",
      "performs well on large sets of data. The performance can be improved by changing the \n",
      "number of hidden neurons and the learning rate. Because of its iterative training and gradient-\n",
      "based training, the general speed is far slower than required, so it takes a significant amount \n",
      "of time to train on an extensive set of data. We cannot say that there is a whole network for \n",
      "every kind of database out there. So keep testing your data on multiple neural networks and \n",
      "see what fits the best.  \n",
      " \n",
      "References\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper explores Backpropagation, a fundamental algorithm used to train artificial neural networks (ANNs). \n",
      "\n",
      "Backpropagation is a multilayered, feedforward neural network training method widely used for supervised learning. It works by adjusting the weights within the network to minimize the difference between predicted and actual outputs. This process involves calculating the error at each layer and propagating it back through the network to adjust the weights accordingly.  Backpropagation is essentially the method for tweaking the weights of a neural network based on the error value calculated in the previous epoch.  This fine-tuning of weights allows for a reduction in the error value and improves the model's reliability by enhancing its predictions.  \n",
      "\n",
      "The history of Backpropagation dates back to the 1960s with early concepts in control theory, but it gained prominence in the 1980s with the work of Rumelhart, Hinton, and Williams, who solidified its application in neural networks.  By the 1990s, Backpropagation had proven its effectiveness, with Wan achieving a breakthrough in pattern recognition using the algorithm.\n",
      "\n",
      "Backpropagation's importance lies in its ability to enable ANNs to learn complex patterns and relationships from data, leading to applications in various fields like image recognition, speech processing, prediction modeling, and more.  The algorithm works by iteratively adjusting the weights of connections between neurons in a network based on the difference between the network's output and the desired output. This process, often visualized as \"feeding forward\" input through the network and then \"backpropagating\" the error signal to adjust weights, allows the network to learn and improve its performance over time.  \n",
      "\n",
      "**Applications:**\n",
      "\n",
      "Backpropagation is used in a variety of applications, including:\n",
      "\n",
      "* **Classification Problems:**  Categorizing data points into predefined classes.\n",
      "* **Functional Approximation:**  Approximating the behavior of a function.\n",
      "* **Time-Series Forecasting:** Predicting future values based on historical data.\n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "While powerful, backpropagation has limitations. Its iterative training process can be slow, especially on large datasets.  Finding the optimal network architecture and learning rate requires experimentation.\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "number of hidden neurons and the learning rate. Because of its iterative training and gradient-\n",
      "based training, the general speed is far slower than required, so it takes a significant amount \n",
      "of time to train on an extensive set of data. We cannot say that there is a whole network for \n",
      "every kind of database out there. So keep testing your data on multiple neural networks and \n",
      "see what fits the best.  \n",
      " \n",
      "References \n",
      "[1] Budiharjo S. Triyuni W. Agus Perdana, and H. Tutut, ‚ÄúPredicting tuition fee payment problem using \n",
      "backpropagation neural network model,‚Äù (2018) \n",
      "[2] M. Huan, C. Ming, and Z. Jianwei, ‚ÄúStudy on  the prediction of real estate price index based on hhga -rbf \n",
      "neural network algorithm ,‚Äù International Journal of u - and e -Service, Science and Technology, SERSC \n",
      "Australia, ISSN: 2005-4246 (Print); pp.2207-9718 (Online), vol.8, no.7, July, (2015) DOI: 10.142 57/ijunnes \n",
      "st.2015.8.7.11. \n",
      "[3] A. Muhammad, A. Khubaib Amjad , and H. Mehdi, ‚ÄúApplication of data mining using artificial neural \n",
      "network: survey ,‚Äù International Journal of Database Theory and Application, vol.8, no.1, (2015) DOI: \n",
      "10.14257/ijdta.2015.8.1.25. \n",
      "[4] P. Jong, ‚ÄúThe characteristic function of CoreNet (Multi -level single-layer artificial neural network s),‚Äù Asia-\n",
      "Pacific Journal of Neura l Networks and Its Applications, vol.1, no.1, (2017) DOI: 10.21742/AJNNIA.201 \n",
      "7.1.1.02 \n",
      "[5] L. Wei, ‚ÄúNeural network model for distortion buckling behaviour of cold-formed steel compression members,‚Äù \n",
      "Helsinki University of Technology Laboratory of Steel Structures Publications 16, (2000) \n",
      "[6] The concept of Back -Propagation Learning by examples from the  http://hebb.cis.uoguelph.ca/~skremer \n",
      "/Teachin g/27642/BP/node3.html\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This paper explores Backpropagation, a fundamental algorithm used to train artificial neural networks (ANNs). \n",
      "\n",
      "Backpropagation is a multilayered, feedforward neural network training method widely used for supervised learning. It works by adjusting the weights within the network to minimize the difference between predicted and actual outputs. This process involves calculating the error at each layer and propagating it back through the network to adjust the weights accordingly.  Backpropagation is essentially the method for tweaking the weights of a neural network based on the error value calculated in the previous epoch.  This fine-tuning of weights allows for a reduction in the error value and improves the model's reliability by enhancing its predictions.  \n",
      "\n",
      "The history of Backpropagation dates back to the 1960s with early concepts in control theory, but it gained prominence in the 1980s with the work of Rumelhart, Hinton, and Williams, who solidified its application in neural networks.  By the 1990s, Backpropagation had proven its effectiveness, with Wan achieving a breakthrough in pattern recognition using the algorithm.\n",
      "\n",
      "Backpropagation's importance lies in its ability to enable ANNs to learn complex patterns and relationships from data, leading to applications in various fields like image recognition, speech processing, prediction modeling, and more.  The algorithm works by iteratively adjusting the weights of connections between neurons in a network based on the difference between the network's output and the desired output. This process, often visualized as \"feeding forward\" input through the network and then \"backpropagating\" the error signal to adjust weights, allows the network to learn and improve its performance over time.  \n",
      "\n",
      "**Factors Affecting Performance:**\n",
      "\n",
      "The effectiveness of backpropagation depends on several factors, including the number of hidden neurons in the network and the learning rate.  Finding the optimal values for these parameters often requires experimentation.  \n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "While powerful, backpropagation has limitations. Its iterative training process can be slow, especially on large datasets.  The training time can be significant, making it less suitable for applications requiring real-time performance. Additionally, there is no single \"best\" network architecture for all datasets, so finding the most effective architecture often involves trial and error.\n",
      "\n",
      "\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "Backpropagation is used in a variety of applications, including:\n",
      "\n",
      "* **Classification Problems:**  Categorizing data points into predefined classes.\n",
      "* **Functional Approximation:**  Approximating the behavior of a function.\n",
      "* **Time-Series Forecasting:** Predicting future values based on historical data. \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = chain.run(splitted_docs)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b519fb",
   "metadata": {},
   "source": [
    "## How to check total input token ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba82e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer untuk document yang diload document dengan langchain \n",
    "encoding = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad46de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = sum(len(encoding.encode(doc.page_content)) for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "692d5066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3518"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
